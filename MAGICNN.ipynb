{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chunlok Lo\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, model_selection, ensemble, neighbors, svm, tree, neural_network, base\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import importlib\n",
    "import datetime\n",
    "import mlTest\n",
    "import preprocessing\n",
    "importlib.reload(mlTest)\n",
    "importlib.reload(preprocessing)\n",
    "df = pd.read_csv(\"data/magic04.data\", header=None)\n",
    "df = preprocessing.convert_numeric(df, [10])\n",
    "X, y = preprocessing.XySplit(df, 10)\n",
    "X_train, X_val, X_test, X_train_val, y_train, y_val, y_test, y_train_val = mlTest.train_val_test_split(X, y, train_size=0.8, val_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d9e4115f8143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown backend: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage_ops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_image_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_ops_impl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# TODO(b/31222613): This op may be differentiable, and there may be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# latent bugs here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotDifferentiable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HSVToRGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotDifferentiable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DrawBoundingBoxes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotDifferentiable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SampleDistortedBoundingBox'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mNotDifferentiable\u001b[1;34m(op_type)\u001b[0m\n\u001b[0;32m   1628\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"op_type must be a string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m   \u001b[0m_gradient_registry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\framework\\registry.py\u001b[0m in \u001b[0;36mregister\u001b[1;34m(self, candidate, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m# stack trace is [this_function, Register(), user_function,...]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;31m# so the user function is #2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_registry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0m_TYPE_TAG\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_LOCATION_TAG\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    358\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \"\"\"\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(10,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# hist2 = model.fit(X_train.values, y_train.values, epochs=30, validation_data=(X_val.values, y_val.values), callbacks=[mc])\n",
    "hist = model.fit(X_train.values, y_train.values, epochs=60, validation_data=(X_val.values, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7166140905564766, 0.7071503681590003, 0.7176656152673073, 0.7045215564445994, 0.7555205048572127, 0.7386961097346245, 0.8002103053182207, 0.8038906418061281, 0.8038906418061281, 0.8038906418061281, 0.8023133544891791, 0.799158780356687, 0.8133543642036045, 0.809674027464994, 0.8086225030048663, 0.8128286015974862, 0.7991587804193627, 0.8044164039108405, 0.8044164040988677, 0.8165089383360966, 0.8012618297783484, 0.8028391168445944, 0.8101997898204093, 0.8133543639529015, 0.8159831757299782, 0.8112513147819431, 0.8207150369287164, 0.8101997900084367, 0.8107255524265278, 0.8065194535832049, 0.8138801263083169, 0.818611987507055, 0.8133543641409288, 0.809674027464994, 0.820189274573301, 0.8149316510191476, 0.818611987507055, 0.8165089382734209, 0.818611987507055, 0.818611987507055, 0.8207150369287164, 0.8222923239949624, 0.7986330180012716, 0.8180862251516396, 0.8128286015974862, 0.8196635122178857, 0.8138801265590199, 0.818611987507055, 0.8175604627962243, 0.8144058886637322, 0.817034700440809, 0.818611987507055, 0.8101997898204093, 0.8207150369287164, 0.8159831759180055, 0.8180862251516396, 0.8212407992841317, 0.8133543641409288, 0.817034700440809, 0.8154574133745629]\n",
      "[0.6870399579390115, 0.72482912723449, 0.7271293375394322, 0.716745531019979, 0.7453995793901157, 0.777602523659306, 0.7905494216614091, 0.8079652996845426, 0.81164563617245, 0.8159174553101998, 0.8178233438485805, 0.818151945320715, 0.8220951629863302, 0.8247896950578338, 0.821437960042061, 0.8219637223974764, 0.8241324921135647, 0.824723974763407, 0.8249868559411146, 0.826301261829653, 0.8284043112513144, 0.8274185068349106, 0.8256440588853838, 0.8280757097791798, 0.8261698212407992, 0.8272213459516299, 0.8247896950578338, 0.827089905362776, 0.824723974763407, 0.8291272344900105, 0.8269584647739222, 0.8269584647739222, 0.8283385909568874, 0.8276813880126183, 0.8281414300736067, 0.8292586750788643, 0.8290615141955836, 0.8283385909568874, 0.8296529968454258, 0.8280099894847529, 0.8305730809674028, 0.8272213459516299, 0.830375920084122, 0.8280099894847529, 0.8280757097791798, 0.8289300736067298, 0.8257097791798107, 0.8296529968454258, 0.830375920084122, 0.8306388012618297, 0.829587276550999, 0.8292586750788643, 0.8313617245005258, 0.829587276550999, 0.8326761303890642, 0.829521556256572, 0.8291272344900105, 0.828732912723449, 0.8291272344900105, 0.8301130389064143]\n"
     ]
    }
   ],
   "source": [
    "plt.plot(range(0, 60), hist.history['val_acc'], label='validation accuracy')\n",
    "plt.plot(range(0, 60), hist.history['acc'], label='train accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(hist.history['val_acc'])\n",
    "print(hist.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8217665615141956\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test.values)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 3s 195us/step - loss: 0.5919 - acc: 0.6923 - val_loss: 0.4919 - val_acc: 0.7697\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.4608 - acc: 0.7926 - val_loss: 0.4890 - val_acc: 0.7787\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.4569 - acc: 0.7917 - val_loss: 0.4918 - val_acc: 0.7739\n",
      "Epoch 4/60\n",
      "13536/15216 [=========================>....] - ETA: 0s - loss: 0.4533 - acc: 0.7958"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-3853021ad391>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mX_ICA_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mica\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_ICA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_ICA_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(10,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA()\n",
    "X_ICA = ica.fit_transform(X_train)\n",
    "X_ICA_val = ica.transform(X_val)\n",
    "\n",
    "hist = model.fit(X_ICA, y_train.values, epochs=60, validation_data=(X_ICA_val, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6614090435512434, 0.7749737121956081, 0.7807570981051769, 0.7828601475268383, 0.7844374344050571, 0.7786540486835155, 0.7844374344050571, 0.781282860272565, 0.7812828604605923, 0.7812828604605923, 0.7854889591158878, 0.782334385171423, 0.7828601475268383, 0.7912723452134839, 0.7981072558338835, 0.7965299687676375, 0.7939011569905607, 0.7949526817013914, 0.8054679288096984, 0.8023133546772063]\n",
      "[0.65017087276551, 0.7568349106203995, 0.7928496319663512, 0.7939668769716088, 0.794689800210305, 0.7951498422712934, 0.7964642481598317, 0.7956756046267087, 0.7965956887486856, 0.7967928496319664, 0.799553101997897, 0.8018533123028391, 0.8027076761303891, 0.8038906414300736, 0.8031677181913774, 0.8098711882229233, 0.81164563617245, 0.8153916929547844, 0.8183491062039958, 0.8211750788643533]\n"
     ]
    }
   ],
   "source": [
    "plt.plot(range(0, 20), hist.history['val_acc'], label='validation accuracy')\n",
    "plt.plot(range(0, 20), hist.history['acc'], label='train accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(hist.history['val_acc'])\n",
    "print(hist.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/20\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.3778 - acc: 0.8333 - val_loss: 0.4020 - val_acc: 0.8149\n",
      "Epoch 2/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3747 - acc: 0.8356 - val_loss: 0.3980 - val_acc: 0.8165\n",
      "Epoch 3/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3711 - acc: 0.8389 - val_loss: 0.3973 - val_acc: 0.8197\n",
      "Epoch 4/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3678 - acc: 0.8392 - val_loss: 0.3955 - val_acc: 0.8228\n",
      "Epoch 5/20\n",
      "15216/15216 [==============================] - 2s 143us/step - loss: 0.3643 - acc: 0.8431 - val_loss: 0.3900 - val_acc: 0.8260\n",
      "Epoch 6/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3622 - acc: 0.8444 - val_loss: 0.3872 - val_acc: 0.8218\n",
      "Epoch 7/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3597 - acc: 0.8463 - val_loss: 0.3861 - val_acc: 0.8270\n",
      "Epoch 8/20\n",
      "15216/15216 [==============================] - 2s 143us/step - loss: 0.3575 - acc: 0.8460 - val_loss: 0.3856 - val_acc: 0.8260\n",
      "Epoch 9/20\n",
      "15216/15216 [==============================] - 2s 143us/step - loss: 0.3559 - acc: 0.8510 - val_loss: 0.3778 - val_acc: 0.8291\n",
      "Epoch 10/20\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.3541 - acc: 0.8490 - val_loss: 0.3752 - val_acc: 0.8370\n",
      "Epoch 11/20\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3519 - acc: 0.8517 - val_loss: 0.3766 - val_acc: 0.8307\n",
      "Epoch 12/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3499 - acc: 0.8521 - val_loss: 0.3758 - val_acc: 0.8286\n",
      "Epoch 13/20\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3483 - acc: 0.8519 - val_loss: 0.3707 - val_acc: 0.8407\n",
      "Epoch 14/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3468 - acc: 0.8539 - val_loss: 0.3756 - val_acc: 0.8344\n",
      "Epoch 15/20\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.3455 - acc: 0.8555 - val_loss: 0.3691 - val_acc: 0.8375\n",
      "Epoch 16/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3447 - acc: 0.8547 - val_loss: 0.3667 - val_acc: 0.8375\n",
      "Epoch 17/20\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3438 - acc: 0.8555 - val_loss: 0.3651 - val_acc: 0.8396\n",
      "Epoch 18/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3420 - acc: 0.8562 - val_loss: 0.3618 - val_acc: 0.8449\n",
      "Epoch 19/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3412 - acc: 0.8568 - val_loss: 0.3628 - val_acc: 0.8417\n",
      "Epoch 20/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3400 - acc: 0.8568 - val_loss: 0.3585 - val_acc: 0.8507\n"
     ]
    }
   ],
   "source": [
    "hist1 = model.fit(X_ICA, y_train.values, epochs=20, validation_data=(X_ICA_val, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8422712933753943\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(ica.transform(X_test))\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3389 - acc: 0.8591 - val_loss: 0.3587 - val_acc: 0.8470\n",
      "Epoch 2/20\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.3377 - acc: 0.8578 - val_loss: 0.3566 - val_acc: 0.8454\n",
      "Epoch 3/20\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3379 - acc: 0.8573 - val_loss: 0.3551 - val_acc: 0.8481\n",
      "Epoch 4/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3354 - acc: 0.8602 - val_loss: 0.3552 - val_acc: 0.8533\n",
      "Epoch 5/20\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.3350 - acc: 0.8595 - val_loss: 0.3588 - val_acc: 0.8454\n",
      "Epoch 6/20\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.3334 - acc: 0.8601 - val_loss: 0.3526 - val_acc: 0.8496\n",
      "Epoch 7/20\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.3328 - acc: 0.8623 - val_loss: 0.3547 - val_acc: 0.8470\n",
      "Epoch 8/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3315 - acc: 0.8614 - val_loss: 0.3670 - val_acc: 0.8438\n",
      "Epoch 9/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3303 - acc: 0.8625 - val_loss: 0.3477 - val_acc: 0.8549\n",
      "Epoch 10/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3295 - acc: 0.8624 - val_loss: 0.3499 - val_acc: 0.8554\n",
      "Epoch 11/20\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.3288 - acc: 0.8634 - val_loss: 0.3557 - val_acc: 0.8496\n",
      "Epoch 12/20\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3295 - acc: 0.8628 - val_loss: 0.3457 - val_acc: 0.8575\n",
      "Epoch 13/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3288 - acc: 0.8624 - val_loss: 0.3447 - val_acc: 0.8591\n",
      "Epoch 14/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3281 - acc: 0.8640 - val_loss: 0.3466 - val_acc: 0.8549\n",
      "Epoch 15/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3272 - acc: 0.8644 - val_loss: 0.3434 - val_acc: 0.8565\n",
      "Epoch 16/20\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.3265 - acc: 0.8667 - val_loss: 0.3453 - val_acc: 0.8586\n",
      "Epoch 17/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3263 - acc: 0.8659 - val_loss: 0.3466 - val_acc: 0.8549\n",
      "Epoch 18/20\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.3257 - acc: 0.8652 - val_loss: 0.3444 - val_acc: 0.8601\n",
      "Epoch 19/20\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3247 - acc: 0.8666 - val_loss: 0.3430 - val_acc: 0.8591\n",
      "Epoch 20/20\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3256 - acc: 0.8664 - val_loss: 0.3494 - val_acc: 0.8544\n"
     ]
    }
   ],
   "source": [
    "hist1 = model.fit(X_ICA, y_train.values, epochs=20, validation_data=(X_ICA_val, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8669821240799159\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(ica.transform(X_test))\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 1.0187 - acc: 0.7009 - val_loss: 0.5317 - val_acc: 0.7660\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.4672 - acc: 0.7881 - val_loss: 0.4872 - val_acc: 0.7897\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.4364 - acc: 0.8007 - val_loss: 0.4714 - val_acc: 0.7944\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.4194 - acc: 0.8091 - val_loss: 0.4498 - val_acc: 0.8091\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4096 - acc: 0.8134 - val_loss: 0.4523 - val_acc: 0.7981\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.4067 - acc: 0.8187 - val_loss: 0.4464 - val_acc: 0.8044\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3974 - acc: 0.8233 - val_loss: 0.4401 - val_acc: 0.8181\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3939 - acc: 0.8266 - val_loss: 0.4256 - val_acc: 0.8113\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3916 - acc: 0.8283 - val_loss: 0.4226 - val_acc: 0.8176\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3885 - acc: 0.8311 - val_loss: 0.4232 - val_acc: 0.8212\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.3858 - acc: 0.8323 - val_loss: 0.4121 - val_acc: 0.8260\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3816 - acc: 0.8339 - val_loss: 0.4085 - val_acc: 0.8149\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.3799 - acc: 0.8337 - val_loss: 0.4140 - val_acc: 0.8165\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3771 - acc: 0.8366 - val_loss: 0.4093 - val_acc: 0.8149\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.3742 - acc: 0.8388 - val_loss: 0.4131 - val_acc: 0.8176\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3750 - acc: 0.8395 - val_loss: 0.4087 - val_acc: 0.8181\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3713 - acc: 0.8389 - val_loss: 0.3965 - val_acc: 0.8249\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.3689 - acc: 0.8426 - val_loss: 0.3970 - val_acc: 0.8265\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3673 - acc: 0.8427 - val_loss: 0.3971 - val_acc: 0.8339\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.3648 - acc: 0.8452 - val_loss: 0.4053 - val_acc: 0.8228\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.3631 - acc: 0.8471 - val_loss: 0.3944 - val_acc: 0.8260\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3618 - acc: 0.8473 - val_loss: 0.3913 - val_acc: 0.8260\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.3577 - acc: 0.8487 - val_loss: 0.3871 - val_acc: 0.8312\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3567 - acc: 0.8466 - val_loss: 0.3910 - val_acc: 0.8297\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.3528 - acc: 0.8531 - val_loss: 0.3841 - val_acc: 0.8312\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.3512 - acc: 0.8516 - val_loss: 0.3764 - val_acc: 0.8423\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3510 - acc: 0.8509 - val_loss: 0.3705 - val_acc: 0.8375\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3465 - acc: 0.8515 - val_loss: 0.3803 - val_acc: 0.8386\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3438 - acc: 0.8565 - val_loss: 0.3706 - val_acc: 0.8417\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3443 - acc: 0.8544 - val_loss: 0.3865 - val_acc: 0.8302\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3399 - acc: 0.8578 - val_loss: 0.3734 - val_acc: 0.8381\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3377 - acc: 0.8570 - val_loss: 0.3792 - val_acc: 0.8365\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3364 - acc: 0.8584 - val_loss: 0.3682 - val_acc: 0.8417\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3350 - acc: 0.8585 - val_loss: 0.3653 - val_acc: 0.8449\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3333 - acc: 0.8594 - val_loss: 0.3723 - val_acc: 0.8417\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3322 - acc: 0.8609 - val_loss: 0.3669 - val_acc: 0.8428\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3301 - acc: 0.8620 - val_loss: 0.3694 - val_acc: 0.8481\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3305 - acc: 0.8599 - val_loss: 0.3643 - val_acc: 0.8486\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3281 - acc: 0.8627 - val_loss: 0.3690 - val_acc: 0.8496\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.3291 - acc: 0.8621 - val_loss: 0.3580 - val_acc: 0.8559\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3265 - acc: 0.8638 - val_loss: 0.3533 - val_acc: 0.8512\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3248 - acc: 0.8643 - val_loss: 0.3530 - val_acc: 0.8580\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.3245 - acc: 0.8643 - val_loss: 0.3879 - val_acc: 0.8344\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3247 - acc: 0.8649 - val_loss: 0.3537 - val_acc: 0.8507\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3215 - acc: 0.8643 - val_loss: 0.3638 - val_acc: 0.8507\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3239 - acc: 0.8636 - val_loss: 0.3559 - val_acc: 0.8580\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3223 - acc: 0.8644 - val_loss: 0.3590 - val_acc: 0.8517\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3199 - acc: 0.8665 - val_loss: 0.3586 - val_acc: 0.8580\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3187 - acc: 0.8658 - val_loss: 0.3480 - val_acc: 0.8586\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3191 - acc: 0.8643 - val_loss: 0.3458 - val_acc: 0.8565\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3186 - acc: 0.8653 - val_loss: 0.3581 - val_acc: 0.8559\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3199 - acc: 0.8656 - val_loss: 0.3553 - val_acc: 0.8512\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3189 - acc: 0.8650 - val_loss: 0.3567 - val_acc: 0.8523\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3178 - acc: 0.8672 - val_loss: 0.3494 - val_acc: 0.8565\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3168 - acc: 0.8667 - val_loss: 0.3499 - val_acc: 0.8586\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3153 - acc: 0.8667 - val_loss: 0.3673 - val_acc: 0.8460\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3164 - acc: 0.8672 - val_loss: 0.3544 - val_acc: 0.8517\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3154 - acc: 0.8672 - val_loss: 0.3514 - val_acc: 0.8554\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3150 - acc: 0.8676 - val_loss: 0.3531 - val_acc: 0.8407\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3139 - acc: 0.8667 - val_loss: 0.3451 - val_acc: 0.8549\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(10,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "X_train_dim = pca.fit_transform(X_train)\n",
    "X_val_dim = pca.transform(X_val)\n",
    "\n",
    "hist = model.fit(X_train_dim, y_train.values, epochs=60, validation_data=(X_val_dim, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8590956887486856\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(pca.transform(X_test))\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 3s 194us/step - loss: 0.9141 - acc: 0.6678 - val_loss: 0.5691 - val_acc: 0.7024\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.5114 - acc: 0.7442 - val_loss: 0.4990 - val_acc: 0.7571\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.4707 - acc: 0.7722 - val_loss: 0.4785 - val_acc: 0.7708\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4526 - acc: 0.7831 - val_loss: 0.4655 - val_acc: 0.7834\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4409 - acc: 0.7957 - val_loss: 0.4543 - val_acc: 0.7965\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.4321 - acc: 0.8040 - val_loss: 0.4437 - val_acc: 0.8018\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.4231 - acc: 0.8105 - val_loss: 0.4315 - val_acc: 0.8086\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 2s 153us/step - loss: 0.4177 - acc: 0.8127 - val_loss: 0.4356 - val_acc: 0.8060\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.4141 - acc: 0.8186 - val_loss: 0.4280 - val_acc: 0.8107\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.4087 - acc: 0.8214 - val_loss: 0.4243 - val_acc: 0.8139\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.4058 - acc: 0.8233 - val_loss: 0.4211 - val_acc: 0.8139\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.4026 - acc: 0.8256 - val_loss: 0.4181 - val_acc: 0.8113\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.3999 - acc: 0.8233 - val_loss: 0.4197 - val_acc: 0.8128\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.3981 - acc: 0.8260 - val_loss: 0.4329 - val_acc: 0.8065\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.3993 - acc: 0.8249 - val_loss: 0.4292 - val_acc: 0.8081\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.3971 - acc: 0.8278 - val_loss: 0.4230 - val_acc: 0.8118\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.3958 - acc: 0.8274 - val_loss: 0.4172 - val_acc: 0.8113\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.3967 - acc: 0.8264 - val_loss: 0.4241 - val_acc: 0.8113\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.3954 - acc: 0.8295 - val_loss: 0.4251 - val_acc: 0.8086\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.3969 - acc: 0.8299 - val_loss: 0.4233 - val_acc: 0.8076\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.3959 - acc: 0.8283 - val_loss: 0.4170 - val_acc: 0.8097\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.3965 - acc: 0.8277 - val_loss: 0.4252 - val_acc: 0.8044\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 2s 156us/step - loss: 0.3930 - acc: 0.8287 - val_loss: 0.4139 - val_acc: 0.8176\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.3948 - acc: 0.8297 - val_loss: 0.4232 - val_acc: 0.8139\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.3930 - acc: 0.8295 - val_loss: 0.4249 - val_acc: 0.8113\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3932 - acc: 0.8297 - val_loss: 0.4154 - val_acc: 0.8091\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.3928 - acc: 0.8296 - val_loss: 0.4477 - val_acc: 0.7944\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3905 - acc: 0.8295 - val_loss: 0.4197 - val_acc: 0.8081\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.3907 - acc: 0.8310 - val_loss: 0.4182 - val_acc: 0.8144\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3900 - acc: 0.8295 - val_loss: 0.4196 - val_acc: 0.8102\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.3912 - acc: 0.8300 - val_loss: 0.4141 - val_acc: 0.8134\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3898 - acc: 0.8302 - val_loss: 0.4234 - val_acc: 0.8155\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 2s 156us/step - loss: 0.3889 - acc: 0.8321 - val_loss: 0.4167 - val_acc: 0.8134\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 2s 156us/step - loss: 0.3887 - acc: 0.8312 - val_loss: 0.4212 - val_acc: 0.8081\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 2s 156us/step - loss: 0.3906 - acc: 0.8287 - val_loss: 0.4209 - val_acc: 0.8060\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.3901 - acc: 0.8302 - val_loss: 0.4171 - val_acc: 0.8139\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.3876 - acc: 0.8309 - val_loss: 0.4251 - val_acc: 0.8086\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.3881 - acc: 0.8302 - val_loss: 0.4520 - val_acc: 0.8002\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.3886 - acc: 0.8304 - val_loss: 0.4195 - val_acc: 0.8113\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3884 - acc: 0.8306 - val_loss: 0.4205 - val_acc: 0.8139\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.3888 - acc: 0.8299 - val_loss: 0.4193 - val_acc: 0.8139\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.3877 - acc: 0.8315 - val_loss: 0.4149 - val_acc: 0.8139\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.3878 - acc: 0.8309 - val_loss: 0.4126 - val_acc: 0.8144\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.3881 - acc: 0.8310 - val_loss: 0.4200 - val_acc: 0.8128\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.3872 - acc: 0.8279 - val_loss: 0.4191 - val_acc: 0.8128\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.3873 - acc: 0.8304 - val_loss: 0.4152 - val_acc: 0.8170\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3846 - acc: 0.8342 - val_loss: 0.4434 - val_acc: 0.7939\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.3867 - acc: 0.8299 - val_loss: 0.4150 - val_acc: 0.8165\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.3861 - acc: 0.8335 - val_loss: 0.4107 - val_acc: 0.8144\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 167us/step - loss: 0.3854 - acc: 0.8330 - val_loss: 0.4140 - val_acc: 0.8149\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.3854 - acc: 0.8334 - val_loss: 0.4156 - val_acc: 0.8181\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3860 - acc: 0.8326 - val_loss: 0.4338 - val_acc: 0.8107\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3891 - acc: 0.8295 - val_loss: 0.4075 - val_acc: 0.8155\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.3856 - acc: 0.8331 - val_loss: 0.4074 - val_acc: 0.8202\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3841 - acc: 0.8318 - val_loss: 0.4123 - val_acc: 0.8149\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.3854 - acc: 0.8334 - val_loss: 0.4190 - val_acc: 0.8176\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3857 - acc: 0.8325 - val_loss: 0.4118 - val_acc: 0.8165\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.3843 - acc: 0.8349 - val_loss: 0.4081 - val_acc: 0.8176\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.3824 - acc: 0.8335 - val_loss: 0.4078 - val_acc: 0.8176\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 2s 156us/step - loss: 0.3839 - acc: 0.8323 - val_loss: 0.4088 - val_acc: 0.8191\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(10,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn import random_projection\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=10, random_state=0)\n",
    "X_train_dim = transformer.fit_transform(X_train)\n",
    "X_val_dim = transformer.transform(X_val)\n",
    "\n",
    "hist = model.fit(X_train_dim, y_train.values, epochs=60, validation_data=(X_val_dim, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8275499474237644\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(transformer.transform(X_test))\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 3s 209us/step - loss: 1.4254 - acc: 0.7119 - val_loss: 0.6012 - val_acc: 0.7576\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.5197 - acc: 0.7852 - val_loss: 0.5202 - val_acc: 0.7666\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4682 - acc: 0.7986 - val_loss: 0.4775 - val_acc: 0.7797\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4433 - acc: 0.8093 - val_loss: 0.4605 - val_acc: 0.7886\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.4308 - acc: 0.8107 - val_loss: 0.4478 - val_acc: 0.8018\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.4215 - acc: 0.8156 - val_loss: 0.4546 - val_acc: 0.8023\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4171 - acc: 0.8170 - val_loss: 0.4422 - val_acc: 0.7981\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4141 - acc: 0.8177 - val_loss: 0.4360 - val_acc: 0.8034\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4116 - acc: 0.8191 - val_loss: 0.4375 - val_acc: 0.8060\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.4088 - acc: 0.8199 - val_loss: 0.4490 - val_acc: 0.8023\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4072 - acc: 0.8197 - val_loss: 0.4466 - val_acc: 0.8086\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4054 - acc: 0.8206 - val_loss: 0.4368 - val_acc: 0.8055\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4055 - acc: 0.8221 - val_loss: 0.4318 - val_acc: 0.8102\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.4049 - acc: 0.8213 - val_loss: 0.4363 - val_acc: 0.7971\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4027 - acc: 0.8215 - val_loss: 0.4256 - val_acc: 0.8102\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.4025 - acc: 0.8233 - val_loss: 0.4314 - val_acc: 0.7997\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4019 - acc: 0.8230 - val_loss: 0.4280 - val_acc: 0.8091\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4010 - acc: 0.8226 - val_loss: 0.4385 - val_acc: 0.8081\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4006 - acc: 0.8226 - val_loss: 0.4327 - val_acc: 0.8034\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4001 - acc: 0.8237 - val_loss: 0.4298 - val_acc: 0.8060\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3983 - acc: 0.8259 - val_loss: 0.4272 - val_acc: 0.8086\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3982 - acc: 0.8256 - val_loss: 0.4427 - val_acc: 0.7992\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3981 - acc: 0.8249 - val_loss: 0.4299 - val_acc: 0.8070\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3978 - acc: 0.8245 - val_loss: 0.4278 - val_acc: 0.8123\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 167us/step - loss: 0.3969 - acc: 0.8262 - val_loss: 0.4283 - val_acc: 0.8023\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.3955 - acc: 0.8274 - val_loss: 0.4290 - val_acc: 0.8070\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 167us/step - loss: 0.3968 - acc: 0.8245 - val_loss: 0.4286 - val_acc: 0.8076\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.3962 - acc: 0.8263 - val_loss: 0.4275 - val_acc: 0.8044\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.3954 - acc: 0.8263 - val_loss: 0.4265 - val_acc: 0.8081\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.3944 - acc: 0.8263 - val_loss: 0.4280 - val_acc: 0.8070\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3947 - acc: 0.8259 - val_loss: 0.4324 - val_acc: 0.8107\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3949 - acc: 0.8264 - val_loss: 0.4291 - val_acc: 0.8055\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3941 - acc: 0.8266 - val_loss: 0.4279 - val_acc: 0.8097\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3951 - acc: 0.8287 - val_loss: 0.4265 - val_acc: 0.8055\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3947 - acc: 0.8256 - val_loss: 0.4266 - val_acc: 0.8034\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3953 - acc: 0.8251 - val_loss: 0.4255 - val_acc: 0.8044\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3937 - acc: 0.8280 - val_loss: 0.4244 - val_acc: 0.8076\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.3932 - acc: 0.8260 - val_loss: 0.4263 - val_acc: 0.8081\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 2s 156us/step - loss: 0.3941 - acc: 0.8271 - val_loss: 0.4217 - val_acc: 0.8070\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.3935 - acc: 0.8272 - val_loss: 0.4284 - val_acc: 0.8102\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.3929 - acc: 0.8262 - val_loss: 0.4333 - val_acc: 0.8049\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.3936 - acc: 0.8260 - val_loss: 0.4306 - val_acc: 0.8097\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.3933 - acc: 0.8279 - val_loss: 0.4299 - val_acc: 0.8023\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.3936 - acc: 0.8267 - val_loss: 0.4354 - val_acc: 0.8049\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3921 - acc: 0.8268 - val_loss: 0.4301 - val_acc: 0.8055\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3925 - acc: 0.8270 - val_loss: 0.4238 - val_acc: 0.8102\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3916 - acc: 0.8271 - val_loss: 0.4259 - val_acc: 0.8065\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3925 - acc: 0.8283 - val_loss: 0.4290 - val_acc: 0.8076\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.3931 - acc: 0.8274 - val_loss: 0.4279 - val_acc: 0.8097\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3918 - acc: 0.8267 - val_loss: 0.4302 - val_acc: 0.8065\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3913 - acc: 0.8275 - val_loss: 0.4239 - val_acc: 0.8128\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3903 - acc: 0.8275 - val_loss: 0.4361 - val_acc: 0.8102\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3921 - acc: 0.8280 - val_loss: 0.4298 - val_acc: 0.8076\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3906 - acc: 0.8272 - val_loss: 0.4266 - val_acc: 0.8091\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3908 - acc: 0.8272 - val_loss: 0.4277 - val_acc: 0.8070\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3910 - acc: 0.8282 - val_loss: 0.4259 - val_acc: 0.8076\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.3910 - acc: 0.8266 - val_loss: 0.4248 - val_acc: 0.8070\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3906 - acc: 0.8296 - val_loss: 0.4225 - val_acc: 0.8113\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.3900 - acc: 0.8279 - val_loss: 0.4262 - val_acc: 0.8065\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3905 - acc: 0.8285 - val_loss: 0.4308 - val_acc: 0.8086\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "X_train_dim = pca.fit_transform(X_train)\n",
    "X_val_dim = pca.transform(X_val)\n",
    "\n",
    "hist = model.fit(X_train_dim, y_train.values, epochs=60, validation_data=(X_val_dim, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81335436382755\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(pca.transform(X_test))\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 4s 243us/step - loss: 0.8427 - acc: 0.7167 - val_loss: 0.5007 - val_acc: 0.7729\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4559 - acc: 0.7963 - val_loss: 0.4689 - val_acc: 0.7913\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4327 - acc: 0.8080 - val_loss: 0.4565 - val_acc: 0.7965\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.4226 - acc: 0.8134 - val_loss: 0.4425 - val_acc: 0.8023\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.4179 - acc: 0.8162 - val_loss: 0.4450 - val_acc: 0.7955\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.4136 - acc: 0.8164 - val_loss: 0.4472 - val_acc: 0.7976\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.4116 - acc: 0.8181 - val_loss: 0.4400 - val_acc: 0.8002\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.4091 - acc: 0.8172 - val_loss: 0.4426 - val_acc: 0.8007\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.4076 - acc: 0.8187 - val_loss: 0.4466 - val_acc: 0.7981\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.4064 - acc: 0.8206 - val_loss: 0.4457 - val_acc: 0.7971\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.4055 - acc: 0.8211 - val_loss: 0.4360 - val_acc: 0.8060\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.4033 - acc: 0.8220 - val_loss: 0.4365 - val_acc: 0.8039\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.4021 - acc: 0.8226 - val_loss: 0.4358 - val_acc: 0.8049\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.4026 - acc: 0.8222 - val_loss: 0.4313 - val_acc: 0.8091\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.4009 - acc: 0.8235 - val_loss: 0.4393 - val_acc: 0.8076\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.4010 - acc: 0.8241 - val_loss: 0.4354 - val_acc: 0.8034\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.3998 - acc: 0.8216 - val_loss: 0.4378 - val_acc: 0.8028\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3998 - acc: 0.8229 - val_loss: 0.4425 - val_acc: 0.8044\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 167us/step - loss: 0.3997 - acc: 0.8241 - val_loss: 0.4308 - val_acc: 0.8070\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.3990 - acc: 0.8247 - val_loss: 0.4348 - val_acc: 0.8060\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.3987 - acc: 0.8263 - val_loss: 0.4351 - val_acc: 0.8065\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.3984 - acc: 0.8243 - val_loss: 0.4310 - val_acc: 0.8065\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.3976 - acc: 0.8266 - val_loss: 0.4348 - val_acc: 0.8049\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.3978 - acc: 0.8245 - val_loss: 0.4374 - val_acc: 0.8028\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.3990 - acc: 0.8252 - val_loss: 0.4374 - val_acc: 0.8055\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.3978 - acc: 0.8244 - val_loss: 0.4378 - val_acc: 0.8049\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.3972 - acc: 0.8256 - val_loss: 0.4342 - val_acc: 0.8060\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.3971 - acc: 0.8265 - val_loss: 0.4340 - val_acc: 0.8055\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.3968 - acc: 0.8252 - val_loss: 0.4342 - val_acc: 0.8065\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3964 - acc: 0.8258 - val_loss: 0.4395 - val_acc: 0.8081\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3962 - acc: 0.8253 - val_loss: 0.4338 - val_acc: 0.8070\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3969 - acc: 0.8266 - val_loss: 0.4326 - val_acc: 0.8044\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3961 - acc: 0.8258 - val_loss: 0.4350 - val_acc: 0.8044\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.3964 - acc: 0.8262 - val_loss: 0.4352 - val_acc: 0.8039\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3952 - acc: 0.8264 - val_loss: 0.4322 - val_acc: 0.8076\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.3947 - acc: 0.8249 - val_loss: 0.4302 - val_acc: 0.8081\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3950 - acc: 0.8260 - val_loss: 0.4404 - val_acc: 0.8065\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.3951 - acc: 0.8249 - val_loss: 0.4370 - val_acc: 0.8055\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.3948 - acc: 0.8255 - val_loss: 0.4403 - val_acc: 0.8007\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.3940 - acc: 0.8272 - val_loss: 0.4354 - val_acc: 0.8018\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.3940 - acc: 0.8271 - val_loss: 0.4361 - val_acc: 0.8081\n",
      "Epoch 42/60\n",
      "11840/15216 [======================>.......] - ETA: 0s - loss: 0.3970 - acc: 0.8230"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-05a3fb807877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mX_val_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(6,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6)\n",
    "X_train_dim = pca.fit_transform(X_train)\n",
    "X_val_dim = pca.transform(X_val)\n",
    "\n",
    "hist = model.fit(X_train_dim, y_train.values, epochs=60, validation_data=(X_val_dim, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8222923238696109\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(pca.transform(X_test))\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 3s 212us/step - loss: 0.6273 - acc: 0.6554 - val_loss: 0.5773 - val_acc: 0.7419\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.5087 - acc: 0.7679 - val_loss: 0.4943 - val_acc: 0.7639\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.4682 - acc: 0.7865 - val_loss: 0.4947 - val_acc: 0.7650\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.4619 - acc: 0.7927 - val_loss: 0.4836 - val_acc: 0.7771\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 164us/step - loss: 0.4594 - acc: 0.7928 - val_loss: 0.4818 - val_acc: 0.7802\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.4571 - acc: 0.7951 - val_loss: 0.4862 - val_acc: 0.7781\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4570 - acc: 0.7938 - val_loss: 0.4818 - val_acc: 0.7813\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4551 - acc: 0.7936 - val_loss: 0.4787 - val_acc: 0.7797\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 201us/step - loss: 0.4541 - acc: 0.7964 - val_loss: 0.4783 - val_acc: 0.7802\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 3s 201us/step - loss: 0.4534 - acc: 0.7961 - val_loss: 0.4789 - val_acc: 0.7776\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4528 - acc: 0.7944 - val_loss: 0.4777 - val_acc: 0.7797\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.4518 - acc: 0.7944 - val_loss: 0.4821 - val_acc: 0.7750\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4513 - acc: 0.7958 - val_loss: 0.4785 - val_acc: 0.7813\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 191us/step - loss: 0.4507 - acc: 0.7949 - val_loss: 0.4740 - val_acc: 0.7818\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4506 - acc: 0.7951 - val_loss: 0.4736 - val_acc: 0.7818\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4489 - acc: 0.7964 - val_loss: 0.4736 - val_acc: 0.7787\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 191us/step - loss: 0.4481 - acc: 0.7948 - val_loss: 0.4706 - val_acc: 0.7781\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 193us/step - loss: 0.4483 - acc: 0.7947 - val_loss: 0.4715 - val_acc: 0.7829\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.4471 - acc: 0.7955 - val_loss: 0.4754 - val_acc: 0.7813\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4465 - acc: 0.7953 - val_loss: 0.4714 - val_acc: 0.7823\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.4464 - acc: 0.7932 - val_loss: 0.4725 - val_acc: 0.7829\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4450 - acc: 0.7969 - val_loss: 0.4762 - val_acc: 0.7834\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4447 - acc: 0.7953 - val_loss: 0.4714 - val_acc: 0.7818\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.4443 - acc: 0.7962 - val_loss: 0.4664 - val_acc: 0.7808\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4434 - acc: 0.7971 - val_loss: 0.4660 - val_acc: 0.7792\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.4441 - acc: 0.7946 - val_loss: 0.4666 - val_acc: 0.7781\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4433 - acc: 0.7960 - val_loss: 0.4681 - val_acc: 0.7776\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4425 - acc: 0.7955 - val_loss: 0.4657 - val_acc: 0.7829\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4422 - acc: 0.7969 - val_loss: 0.4656 - val_acc: 0.7792\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4413 - acc: 0.7959 - val_loss: 0.4720 - val_acc: 0.7802\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4417 - acc: 0.7967 - val_loss: 0.4737 - val_acc: 0.7860\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4421 - acc: 0.7973 - val_loss: 0.4674 - val_acc: 0.7860\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4413 - acc: 0.7965 - val_loss: 0.4636 - val_acc: 0.7792\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4410 - acc: 0.7971 - val_loss: 0.4640 - val_acc: 0.7776\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4401 - acc: 0.7972 - val_loss: 0.4644 - val_acc: 0.7797\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4400 - acc: 0.7958 - val_loss: 0.4642 - val_acc: 0.7760\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4412 - acc: 0.7951 - val_loss: 0.4630 - val_acc: 0.7781\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4403 - acc: 0.7963 - val_loss: 0.4637 - val_acc: 0.7818\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4396 - acc: 0.7973 - val_loss: 0.4639 - val_acc: 0.7776\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4393 - acc: 0.7942 - val_loss: 0.4625 - val_acc: 0.7787\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4380 - acc: 0.7963 - val_loss: 0.4625 - val_acc: 0.7776\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4376 - acc: 0.7978 - val_loss: 0.4626 - val_acc: 0.7792\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4371 - acc: 0.7963 - val_loss: 0.4661 - val_acc: 0.7871\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4365 - acc: 0.7975 - val_loss: 0.4624 - val_acc: 0.7776\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4360 - acc: 0.7974 - val_loss: 0.4605 - val_acc: 0.7776\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4345 - acc: 0.7979 - val_loss: 0.4613 - val_acc: 0.7834\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4336 - acc: 0.7983 - val_loss: 0.4611 - val_acc: 0.7865\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4322 - acc: 0.7982 - val_loss: 0.4567 - val_acc: 0.7766\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4313 - acc: 0.7980 - val_loss: 0.4610 - val_acc: 0.7781\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 192us/step - loss: 0.4302 - acc: 0.7999 - val_loss: 0.4546 - val_acc: 0.7787\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4295 - acc: 0.8010 - val_loss: 0.4530 - val_acc: 0.7829\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4278 - acc: 0.7992 - val_loss: 0.4534 - val_acc: 0.7792\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4267 - acc: 0.7991 - val_loss: 0.4494 - val_acc: 0.7865\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.4245 - acc: 0.8010 - val_loss: 0.4470 - val_acc: 0.7865\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4232 - acc: 0.8030 - val_loss: 0.4449 - val_acc: 0.7844\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4213 - acc: 0.8047 - val_loss: 0.4490 - val_acc: 0.7897\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4196 - acc: 0.8061 - val_loss: 0.4423 - val_acc: 0.7897\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4176 - acc: 0.8067 - val_loss: 0.4413 - val_acc: 0.7944\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4162 - acc: 0.8083 - val_loss: 0.4393 - val_acc: 0.7939\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4156 - acc: 0.8085 - val_loss: 0.4393 - val_acc: 0.7976\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=5)\n",
    "X_ICA = ica.fit_transform(X_train)\n",
    "X_ICA_val = ica.transform(X_val)\n",
    "\n",
    "hist = model.fit(X_ICA, y_train.values, epochs=60, validation_data=(X_ICA_val, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8038906414300736\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(ica.transform(X_test))\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 4s 254us/step - loss: 1.0996 - acc: 0.6521 - val_loss: 0.6310 - val_acc: 0.6788\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 3s 199us/step - loss: 0.5882 - acc: 0.7073 - val_loss: 0.5659 - val_acc: 0.7171\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 195us/step - loss: 0.5483 - acc: 0.7290 - val_loss: 0.5512 - val_acc: 0.7219\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.5348 - acc: 0.7404 - val_loss: 0.5462 - val_acc: 0.7387\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.5227 - acc: 0.7516 - val_loss: 0.5337 - val_acc: 0.7450\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.5127 - acc: 0.7614 - val_loss: 0.5502 - val_acc: 0.7355\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.5115 - acc: 0.7594 - val_loss: 0.5203 - val_acc: 0.7434\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.5060 - acc: 0.7645 - val_loss: 0.5126 - val_acc: 0.7560\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.5000 - acc: 0.7671 - val_loss: 0.5329 - val_acc: 0.7513\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4964 - acc: 0.7683 - val_loss: 0.5086 - val_acc: 0.7576\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4924 - acc: 0.7691 - val_loss: 0.5118 - val_acc: 0.7603\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4907 - acc: 0.7698 - val_loss: 0.5192 - val_acc: 0.7450\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4903 - acc: 0.7737 - val_loss: 0.5078 - val_acc: 0.7629\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4889 - acc: 0.7742 - val_loss: 0.5082 - val_acc: 0.7518\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4854 - acc: 0.7780 - val_loss: 0.5337 - val_acc: 0.7455\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4898 - acc: 0.7714 - val_loss: 0.5015 - val_acc: 0.7660\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4867 - acc: 0.7739 - val_loss: 0.4960 - val_acc: 0.7671\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4821 - acc: 0.7779 - val_loss: 0.5092 - val_acc: 0.7581\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4838 - acc: 0.7775 - val_loss: 0.5048 - val_acc: 0.7608\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4848 - acc: 0.7758 - val_loss: 0.5061 - val_acc: 0.7529\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4833 - acc: 0.7756 - val_loss: 0.5116 - val_acc: 0.7492\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4802 - acc: 0.7812 - val_loss: 0.4948 - val_acc: 0.7639\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4823 - acc: 0.7775 - val_loss: 0.4955 - val_acc: 0.7671\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4807 - acc: 0.7813 - val_loss: 0.4949 - val_acc: 0.7592\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4793 - acc: 0.7806 - val_loss: 0.4979 - val_acc: 0.7718\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4808 - acc: 0.7829 - val_loss: 0.4932 - val_acc: 0.7618\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4795 - acc: 0.7823 - val_loss: 0.4887 - val_acc: 0.7671\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4805 - acc: 0.7796 - val_loss: 0.5239 - val_acc: 0.7492\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4778 - acc: 0.7808 - val_loss: 0.5048 - val_acc: 0.7613\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4797 - acc: 0.7805 - val_loss: 0.4893 - val_acc: 0.7666\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4782 - acc: 0.7806 - val_loss: 0.4967 - val_acc: 0.7666\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4804 - acc: 0.7753 - val_loss: 0.4920 - val_acc: 0.7708\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4783 - acc: 0.7798 - val_loss: 0.4933 - val_acc: 0.7671\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4768 - acc: 0.7846 - val_loss: 0.4887 - val_acc: 0.7681\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4775 - acc: 0.7817 - val_loss: 0.5060 - val_acc: 0.7655\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4787 - acc: 0.7801 - val_loss: 0.4931 - val_acc: 0.7650\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4763 - acc: 0.7813 - val_loss: 0.4913 - val_acc: 0.7718\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4768 - acc: 0.7837 - val_loss: 0.4983 - val_acc: 0.7613\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4779 - acc: 0.7815 - val_loss: 0.4905 - val_acc: 0.7681\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4756 - acc: 0.7849 - val_loss: 0.5106 - val_acc: 0.7560\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4754 - acc: 0.7829 - val_loss: 0.4939 - val_acc: 0.7645\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.4748 - acc: 0.7816 - val_loss: 0.4964 - val_acc: 0.7608\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 192us/step - loss: 0.4765 - acc: 0.7833 - val_loss: 0.5001 - val_acc: 0.7639\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.4754 - acc: 0.7857 - val_loss: 0.4968 - val_acc: 0.7634\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.4772 - acc: 0.7826 - val_loss: 0.4965 - val_acc: 0.7603\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.4755 - acc: 0.7854 - val_loss: 0.4958 - val_acc: 0.7650\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 196us/step - loss: 0.4764 - acc: 0.7828 - val_loss: 0.4887 - val_acc: 0.7671\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 209us/step - loss: 0.4747 - acc: 0.7851 - val_loss: 0.4908 - val_acc: 0.7687\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.4734 - acc: 0.7869 - val_loss: 0.4928 - val_acc: 0.7671\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4747 - acc: 0.7860 - val_loss: 0.5077 - val_acc: 0.7534\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4778 - acc: 0.7796 - val_loss: 0.5036 - val_acc: 0.7592\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4736 - acc: 0.7855 - val_loss: 0.4973 - val_acc: 0.7660\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4732 - acc: 0.7856 - val_loss: 0.4892 - val_acc: 0.7676\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4726 - acc: 0.7848 - val_loss: 0.4937 - val_acc: 0.7676\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4742 - acc: 0.7845 - val_loss: 0.4954 - val_acc: 0.7613\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4749 - acc: 0.7810 - val_loss: 0.4924 - val_acc: 0.7645\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4724 - acc: 0.7846 - val_loss: 0.4919 - val_acc: 0.7671\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4726 - acc: 0.7865 - val_loss: 0.4894 - val_acc: 0.7708\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4734 - acc: 0.7855 - val_loss: 0.4968 - val_acc: 0.7634\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4724 - acc: 0.7851 - val_loss: 0.4893 - val_acc: 0.7666\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn import random_projection\n",
    "transformer = random_projection.GaussianRandomProjection(n_components=5, random_state=0)\n",
    "X_train_dim = transformer.fit_transform(X_train)\n",
    "X_val_dim = transformer.transform(X_val)\n",
    "\n",
    "hist = model.fit(X_train_dim, y_train.values, epochs=60, validation_data=(X_val_dim, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6430073606729758\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(ica.transform(X_test))\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 3s 223us/step - loss: 1.1671 - acc: 0.6572 - val_loss: 0.6044 - val_acc: 0.7114\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 153us/step - loss: 0.5555 - acc: 0.7262 - val_loss: 0.5410 - val_acc: 0.7434\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4972 - acc: 0.7791 - val_loss: 0.4984 - val_acc: 0.7860\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4641 - acc: 0.7994 - val_loss: 0.4738 - val_acc: 0.7923\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4487 - acc: 0.8091 - val_loss: 0.4676 - val_acc: 0.7907\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4349 - acc: 0.8162 - val_loss: 0.4552 - val_acc: 0.8023\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4281 - acc: 0.8194 - val_loss: 0.4522 - val_acc: 0.8007\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4227 - acc: 0.8197 - val_loss: 0.4489 - val_acc: 0.8007\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4196 - acc: 0.8207 - val_loss: 0.4462 - val_acc: 0.8044\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4182 - acc: 0.8189 - val_loss: 0.4400 - val_acc: 0.8102\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4131 - acc: 0.8245 - val_loss: 0.4399 - val_acc: 0.8076\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4125 - acc: 0.8218 - val_loss: 0.4357 - val_acc: 0.8091\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 2s 156us/step - loss: 0.4083 - acc: 0.8254 - val_loss: 0.4310 - val_acc: 0.8186\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.4070 - acc: 0.8258 - val_loss: 0.4325 - val_acc: 0.8134\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.4061 - acc: 0.8270 - val_loss: 0.4329 - val_acc: 0.8123\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.4042 - acc: 0.8283 - val_loss: 0.4345 - val_acc: 0.8076\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.4029 - acc: 0.8302 - val_loss: 0.4269 - val_acc: 0.8134\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 2s 153us/step - loss: 0.4019 - acc: 0.8307 - val_loss: 0.4257 - val_acc: 0.8134\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.4026 - acc: 0.8297 - val_loss: 0.4329 - val_acc: 0.8070\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.4014 - acc: 0.8280 - val_loss: 0.4353 - val_acc: 0.8113\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.3993 - acc: 0.8304 - val_loss: 0.4259 - val_acc: 0.8170\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3996 - acc: 0.8313 - val_loss: 0.4294 - val_acc: 0.8212\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.3996 - acc: 0.8306 - val_loss: 0.4265 - val_acc: 0.8149\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.3978 - acc: 0.8304 - val_loss: 0.4381 - val_acc: 0.8055\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.3992 - acc: 0.8306 - val_loss: 0.4288 - val_acc: 0.8149\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.3962 - acc: 0.8295 - val_loss: 0.4284 - val_acc: 0.8186\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3976 - acc: 0.8302 - val_loss: 0.4327 - val_acc: 0.8065\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3959 - acc: 0.8331 - val_loss: 0.4291 - val_acc: 0.8191\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.3957 - acc: 0.8316 - val_loss: 0.4196 - val_acc: 0.8186\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3955 - acc: 0.8317 - val_loss: 0.4235 - val_acc: 0.8123\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3952 - acc: 0.8326 - val_loss: 0.4242 - val_acc: 0.8186\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3938 - acc: 0.8339 - val_loss: 0.4286 - val_acc: 0.8239\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3948 - acc: 0.8304 - val_loss: 0.4235 - val_acc: 0.8228\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.3934 - acc: 0.8319 - val_loss: 0.4231 - val_acc: 0.8181\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 164us/step - loss: 0.3940 - acc: 0.8322 - val_loss: 0.4206 - val_acc: 0.8202\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.3928 - acc: 0.8337 - val_loss: 0.4237 - val_acc: 0.8139\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.3934 - acc: 0.8349 - val_loss: 0.4180 - val_acc: 0.8218\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.3925 - acc: 0.8328 - val_loss: 0.4179 - val_acc: 0.8212\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3938 - acc: 0.8322 - val_loss: 0.4268 - val_acc: 0.8139\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 167us/step - loss: 0.3924 - acc: 0.8335 - val_loss: 0.4316 - val_acc: 0.8207\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 164us/step - loss: 0.3931 - acc: 0.8325 - val_loss: 0.4220 - val_acc: 0.8218\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3910 - acc: 0.8331 - val_loss: 0.4224 - val_acc: 0.8207\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.3927 - acc: 0.8323 - val_loss: 0.4306 - val_acc: 0.8023\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.3916 - acc: 0.8343 - val_loss: 0.4190 - val_acc: 0.8202\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3910 - acc: 0.8333 - val_loss: 0.4296 - val_acc: 0.8123\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - ETA: 0s - loss: 0.3908 - acc: 0.834 - 2s 162us/step - loss: 0.3918 - acc: 0.8339 - val_loss: 0.4203 - val_acc: 0.8254\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.3914 - acc: 0.8330 - val_loss: 0.4233 - val_acc: 0.8123\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3902 - acc: 0.8330 - val_loss: 0.4183 - val_acc: 0.8239\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.3896 - acc: 0.8339 - val_loss: 0.4173 - val_acc: 0.8197\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.3895 - acc: 0.8339 - val_loss: 0.4269 - val_acc: 0.8097\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3916 - acc: 0.8317 - val_loss: 0.4184 - val_acc: 0.8212\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.3886 - acc: 0.8348 - val_loss: 0.4198 - val_acc: 0.8254\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3885 - acc: 0.8348 - val_loss: 0.4241 - val_acc: 0.8113\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.3882 - acc: 0.8355 - val_loss: 0.4231 - val_acc: 0.8086\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.3879 - acc: 0.8346 - val_loss: 0.4214 - val_acc: 0.8223\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3874 - acc: 0.8352 - val_loss: 0.4164 - val_acc: 0.8191\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.3884 - acc: 0.8356 - val_loss: 0.4308 - val_acc: 0.8212\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.3878 - acc: 0.8339 - val_loss: 0.4223 - val_acc: 0.8113\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.3864 - acc: 0.8356 - val_loss: 0.4184 - val_acc: 0.8170\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 167us/step - loss: 0.3869 - acc: 0.8367 - val_loss: 0.4272 - val_acc: 0.8123\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "X_val_dim = fvalue_selector.transform(X_val)\n",
    "\n",
    "hist = model.fit(X_train_dim, y_train.values, epochs=60, validation_data=(X_val_dim, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8275499474237644\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(fvalue_selector.transform(X_test))\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 4s 272us/step - loss: 0.6257 - acc: 0.6598 - val_loss: 0.6128 - val_acc: 0.6882\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.6062 - acc: 0.6925 - val_loss: 0.6128 - val_acc: 0.6882\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.6059 - acc: 0.6925 - val_loss: 0.6130 - val_acc: 0.6882\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.6059 - acc: 0.6925 - val_loss: 0.6129 - val_acc: 0.6882\n",
      "Epoch 5/60\n",
      " 2368/15216 [===>..........................] - ETA: 2s - loss: 0.6184 - acc: 0.6820"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-788a44b6a40a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# print(X_train_kmeans)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_kmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_kmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "# fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "# X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=5)\n",
    "km = km.fit(X_train)\n",
    "X_train_kmeans = km.predict(X_train)\n",
    "X_val_kmeans = km.predict(X_val)\n",
    "X_test_kmeans = km.predict(X_test)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_kmeans, y_train.values, epochs=60, validation_data=(X_val_kmeans, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6887486855941115\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_kmeans)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99628748e-001 1.48928695e-046 3.71252174e-004 1.42255991e-010\n",
      " 7.05858580e-111]\n",
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 4s 258us/step - loss: 0.5065 - acc: 0.7656 - val_loss: 0.4660 - val_acc: 0.7855\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.4534 - acc: 0.7896 - val_loss: 0.4622 - val_acc: 0.7886\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.4518 - acc: 0.7930 - val_loss: 0.4617 - val_acc: 0.7865\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4509 - acc: 0.7914 - val_loss: 0.4609 - val_acc: 0.7913\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.4499 - acc: 0.7923 - val_loss: 0.4600 - val_acc: 0.7881\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4491 - acc: 0.7909 - val_loss: 0.4619 - val_acc: 0.7944\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4483 - acc: 0.7918 - val_loss: 0.4596 - val_acc: 0.7881\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4479 - acc: 0.7917 - val_loss: 0.4613 - val_acc: 0.7939\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4472 - acc: 0.7928 - val_loss: 0.4578 - val_acc: 0.7902\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4463 - acc: 0.7925 - val_loss: 0.4574 - val_acc: 0.7892\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4457 - acc: 0.7925 - val_loss: 0.4582 - val_acc: 0.7897\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.4451 - acc: 0.7939 - val_loss: 0.4588 - val_acc: 0.7886\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.4444 - acc: 0.7942 - val_loss: 0.4577 - val_acc: 0.7928\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.4438 - acc: 0.7938 - val_loss: 0.4569 - val_acc: 0.7934\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.4432 - acc: 0.7940 - val_loss: 0.4552 - val_acc: 0.7902\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.4424 - acc: 0.7940 - val_loss: 0.4546 - val_acc: 0.7913\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.4425 - acc: 0.7951 - val_loss: 0.4546 - val_acc: 0.7934\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.4415 - acc: 0.7956 - val_loss: 0.4538 - val_acc: 0.7902\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.4411 - acc: 0.7950 - val_loss: 0.4546 - val_acc: 0.7913\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.4404 - acc: 0.7951 - val_loss: 0.4556 - val_acc: 0.7897\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4401 - acc: 0.7955 - val_loss: 0.4548 - val_acc: 0.7939\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.4397 - acc: 0.7959 - val_loss: 0.4535 - val_acc: 0.7918\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.4392 - acc: 0.7951 - val_loss: 0.4547 - val_acc: 0.7928\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.4392 - acc: 0.7961 - val_loss: 0.4532 - val_acc: 0.7939\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4386 - acc: 0.7964 - val_loss: 0.4585 - val_acc: 0.7881\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.4386 - acc: 0.7957 - val_loss: 0.4526 - val_acc: 0.7928\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4385 - acc: 0.7963 - val_loss: 0.4527 - val_acc: 0.7918\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.4375 - acc: 0.7965 - val_loss: 0.4520 - val_acc: 0.7923\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4369 - acc: 0.7976 - val_loss: 0.4557 - val_acc: 0.7902\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4369 - acc: 0.7968 - val_loss: 0.4521 - val_acc: 0.7923\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.4370 - acc: 0.7970 - val_loss: 0.4537 - val_acc: 0.7918\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.4364 - acc: 0.7982 - val_loss: 0.4514 - val_acc: 0.7918\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4360 - acc: 0.7975 - val_loss: 0.4515 - val_acc: 0.7913\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4352 - acc: 0.7982 - val_loss: 0.4539 - val_acc: 0.7934\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4350 - acc: 0.7978 - val_loss: 0.4573 - val_acc: 0.7881\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4354 - acc: 0.7979 - val_loss: 0.4513 - val_acc: 0.7934\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4348 - acc: 0.7983 - val_loss: 0.4511 - val_acc: 0.7939\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4347 - acc: 0.7980 - val_loss: 0.4510 - val_acc: 0.7923\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4344 - acc: 0.7986 - val_loss: 0.4506 - val_acc: 0.7934\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4339 - acc: 0.7987 - val_loss: 0.4504 - val_acc: 0.7923\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.4339 - acc: 0.7984 - val_loss: 0.4502 - val_acc: 0.7923\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4337 - acc: 0.7985 - val_loss: 0.4495 - val_acc: 0.7923\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.4332 - acc: 0.7983 - val_loss: 0.4496 - val_acc: 0.7928\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.4327 - acc: 0.8003 - val_loss: 0.4496 - val_acc: 0.7923\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4323 - acc: 0.7997 - val_loss: 0.4503 - val_acc: 0.7928\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4321 - acc: 0.7993 - val_loss: 0.4504 - val_acc: 0.7913\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4323 - acc: 0.7994 - val_loss: 0.4497 - val_acc: 0.7913\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4321 - acc: 0.7995 - val_loss: 0.4497 - val_acc: 0.7918\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.4316 - acc: 0.7999 - val_loss: 0.4502 - val_acc: 0.7923\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4315 - acc: 0.7992 - val_loss: 0.4504 - val_acc: 0.7928\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4317 - acc: 0.7989 - val_loss: 0.4494 - val_acc: 0.7923\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4311 - acc: 0.8012 - val_loss: 0.4489 - val_acc: 0.7918\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.4303 - acc: 0.8005 - val_loss: 0.4484 - val_acc: 0.7950\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4303 - acc: 0.7994 - val_loss: 0.4504 - val_acc: 0.7928\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4302 - acc: 0.8001 - val_loss: 0.4498 - val_acc: 0.7928\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.4306 - acc: 0.7997 - val_loss: 0.4481 - val_acc: 0.7939\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4306 - acc: 0.8004 - val_loss: 0.4530 - val_acc: 0.7886\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4301 - acc: 0.8001 - val_loss: 0.4501 - val_acc: 0.7934\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.4300 - acc: 0.8018 - val_loss: 0.4476 - val_acc: 0.7944\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.4296 - acc: 0.8005 - val_loss: 0.4490 - val_acc: 0.7939\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "# fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "# X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = GaussianMixture(n_components=5)\n",
    "km = km.fit(X_train)\n",
    "X_train_kmeans = km.predict_proba(X_train)\n",
    "X_val_kmeans = km.predict_proba(X_val)\n",
    "X_test_kmeans = km.predict_proba(X_test)\n",
    "\n",
    "print(X_train_kmeans[0])\n",
    "# from keras.utils import to_categorical\n",
    "# X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "# X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "# X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_kmeans, y_train.values, epochs=60, validation_data=(X_val_kmeans, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7975814931650894\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_kmeans)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 4s 267us/step - loss: 0.5547 - acc: 0.7336 - val_loss: 0.4777 - val_acc: 0.7739\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4783 - val_acc: 0.7739\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.4850 - acc: 0.7693 - val_loss: 0.4776 - val_acc: 0.7739\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4772 - val_acc: 0.7739\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4779 - val_acc: 0.7739\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.4850 - acc: 0.7693 - val_loss: 0.4774 - val_acc: 0.7739\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4769 - val_acc: 0.7739\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.4851 - acc: 0.7693 - val_loss: 0.4771 - val_acc: 0.7739\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4777 - val_acc: 0.7739\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4779 - val_acc: 0.7739\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 2s 156us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4775 - val_acc: 0.7739\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 2s 141us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4783 - val_acc: 0.7739\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 2s 139us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4783 - val_acc: 0.7739\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4777 - val_acc: 0.7739\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 2s 143us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4780 - val_acc: 0.7739\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 2s 144us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4776 - val_acc: 0.7739\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4775 - val_acc: 0.7739\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.4850 - acc: 0.7693 - val_loss: 0.4779 - val_acc: 0.7739\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.4846 - acc: 0.7693 - val_loss: 0.4786 - val_acc: 0.7739\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4776 - val_acc: 0.7739\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4778 - val_acc: 0.7739\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4779 - val_acc: 0.7739\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4772 - val_acc: 0.7739\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4776 - val_acc: 0.7739\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4779 - val_acc: 0.7739\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4779 - val_acc: 0.7739\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4777 - val_acc: 0.7739\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4777 - val_acc: 0.7739\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4779 - val_acc: 0.7739\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.4850 - acc: 0.7693 - val_loss: 0.4781 - val_acc: 0.7739\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4775 - val_acc: 0.7739\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4779 - val_acc: 0.7739\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4789 - val_acc: 0.7739\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4775 - val_acc: 0.7739\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4846 - acc: 0.7693 - val_loss: 0.4782 - val_acc: 0.7739\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4777 - val_acc: 0.7739\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.4850 - acc: 0.7693 - val_loss: 0.4776 - val_acc: 0.7739\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4784 - val_acc: 0.7739\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.4846 - acc: 0.7693 - val_loss: 0.4777 - val_acc: 0.7739\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4777 - val_acc: 0.7739\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4776 - val_acc: 0.7739\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 2s 156us/step - loss: 0.4846 - acc: 0.7693 - val_loss: 0.4777 - val_acc: 0.7739\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.4847 - acc: 0.7693 - val_loss: 0.4775 - val_acc: 0.7739\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4846 - acc: 0.7693 - val_loss: 0.4774 - val_acc: 0.7739\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4774 - val_acc: 0.7739\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.4846 - acc: 0.7693 - val_loss: 0.4789 - val_acc: 0.7739\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4773 - val_acc: 0.7739\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.4848 - acc: 0.7693 - val_loss: 0.4780 - val_acc: 0.7739\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4849 - acc: 0.7693 - val_loss: 0.4779 - val_acc: 0.7739\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.4846 - acc: 0.7693 - val_loss: 0.4780 - val_acc: 0.7739\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 2s 153us/step - loss: 0.4846 - acc: 0.7693 - val_loss: 0.4780 - val_acc: 0.7739\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4846 - acc: 0.7693 - val_loss: 0.4780 - val_acc: 0.7739\n",
      "Epoch 53/60\n",
      " 9824/15216 [==================>...........] - ETA: 0s - loss: 0.4833 - acc: 0.7722"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a56cd8842e56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# print(X_train_kmeans)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_kmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_kmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "# fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "# X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = GaussianMixture(n_components=5)\n",
    "km = km.fit(X_train)\n",
    "X_train_kmeans = km.predict(X_train)\n",
    "X_val_kmeans = km.predict(X_val)\n",
    "X_test_kmeans = km.predict(X_test)\n",
    "\n",
    "print(X_train_kmeans[0])\n",
    "from keras.utils import to_categorical\n",
    "X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_kmeans, y_train.values, epochs=60, validation_data=(X_val_kmeans, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7770767613038907\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_kmeans)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chunlok Lo\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\sklearn\\decomposition\\fastica_.py:121: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 4s 276us/step - loss: 0.6092 - acc: 0.6995 - val_loss: 0.6052 - val_acc: 0.6940\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 162us/step - loss: 0.5880 - acc: 0.7008 - val_loss: 0.6048 - val_acc: 0.6940\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.5878 - acc: 0.7017 - val_loss: 0.6047 - val_acc: 0.6940\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.5878 - acc: 0.7017 - val_loss: 0.6053 - val_acc: 0.6940\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.5879 - acc: 0.7017 - val_loss: 0.6050 - val_acc: 0.6940\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.5877 - acc: 0.7017 - val_loss: 0.6064 - val_acc: 0.6940\n",
      "Epoch 7/60\n",
      " 9248/15216 [=================>............] - ETA: 0s - loss: 0.5890 - acc: 0.6999"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-ac4ec118618e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# print(X_train_kmeans)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_kmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_kmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1217\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1219\u001b[1;33m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1220\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values, force)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "# fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "# X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=10)\n",
    "X_ICA = ica.fit_transform(X_train)\n",
    "X_ICA_val = ica.transform(X_val)\n",
    "X_ICA_test = ica.transform(X_test)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = KMeans(n_clusters=5)\n",
    "km = km.fit(X_ICA)\n",
    "X_train_kmeans = km.predict(X_ICA)\n",
    "X_val_kmeans = km.predict(X_ICA_val)\n",
    "X_test_kmeans = km.predict(X_ICA_test)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_kmeans, y_train.values, epochs=60, validation_data=(X_val_kmeans, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7450052576235542\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_kmeans)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 5s 309us/step - loss: 0.5145 - acc: 0.7344 - val_loss: 0.4757 - val_acc: 0.7645\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 3s 196us/step - loss: 0.4528 - acc: 0.7794 - val_loss: 0.4756 - val_acc: 0.7581\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 210us/step - loss: 0.4518 - acc: 0.7812 - val_loss: 0.4761 - val_acc: 0.7734\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 205us/step - loss: 0.4510 - acc: 0.7820 - val_loss: 0.4740 - val_acc: 0.7681\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4498 - acc: 0.7816 - val_loss: 0.4735 - val_acc: 0.7734\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4489 - acc: 0.7824 - val_loss: 0.4736 - val_acc: 0.7676\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 195us/step - loss: 0.4485 - acc: 0.7833 - val_loss: 0.4718 - val_acc: 0.7681\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 193us/step - loss: 0.4472 - acc: 0.7842 - val_loss: 0.4707 - val_acc: 0.7697\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 4s 237us/step - loss: 0.4464 - acc: 0.7856 - val_loss: 0.4712 - val_acc: 0.7776\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 3s 200us/step - loss: 0.4456 - acc: 0.7838 - val_loss: 0.4701 - val_acc: 0.7713\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4445 - acc: 0.7842 - val_loss: 0.4692 - val_acc: 0.7750\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4440 - acc: 0.7854 - val_loss: 0.4684 - val_acc: 0.7755\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.4432 - acc: 0.7850 - val_loss: 0.4661 - val_acc: 0.7755\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4428 - acc: 0.7864 - val_loss: 0.4670 - val_acc: 0.7750\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4421 - acc: 0.7879 - val_loss: 0.4670 - val_acc: 0.7760\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4414 - acc: 0.7844 - val_loss: 0.4686 - val_acc: 0.7734\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4409 - acc: 0.7867 - val_loss: 0.4668 - val_acc: 0.7755\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4412 - acc: 0.7851 - val_loss: 0.4686 - val_acc: 0.7744\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4402 - acc: 0.7873 - val_loss: 0.4687 - val_acc: 0.7755\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.4397 - acc: 0.7866 - val_loss: 0.4679 - val_acc: 0.7755\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 193us/step - loss: 0.4391 - acc: 0.7879 - val_loss: 0.4700 - val_acc: 0.7534\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4391 - acc: 0.7861 - val_loss: 0.4651 - val_acc: 0.7766\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4386 - acc: 0.7880 - val_loss: 0.4673 - val_acc: 0.7776\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4385 - acc: 0.7884 - val_loss: 0.4654 - val_acc: 0.7776\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4382 - acc: 0.7876 - val_loss: 0.4663 - val_acc: 0.7613\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4376 - acc: 0.7881 - val_loss: 0.4640 - val_acc: 0.7787\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 192us/step - loss: 0.4371 - acc: 0.7877 - val_loss: 0.4638 - val_acc: 0.7766\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4372 - acc: 0.7879 - val_loss: 0.4659 - val_acc: 0.7808\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4357 - acc: 0.7870 - val_loss: 0.4674 - val_acc: 0.7766\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4367 - acc: 0.7888 - val_loss: 0.4649 - val_acc: 0.7781\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4360 - acc: 0.7905 - val_loss: 0.4626 - val_acc: 0.7744\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4360 - acc: 0.7881 - val_loss: 0.4645 - val_acc: 0.7797\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4357 - acc: 0.7902 - val_loss: 0.4630 - val_acc: 0.7792\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4353 - acc: 0.7871 - val_loss: 0.4620 - val_acc: 0.7787\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4349 - acc: 0.7911 - val_loss: 0.4622 - val_acc: 0.7781\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4348 - acc: 0.7872 - val_loss: 0.4631 - val_acc: 0.7771\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4341 - acc: 0.7902 - val_loss: 0.4649 - val_acc: 0.7608\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4341 - acc: 0.7913 - val_loss: 0.4654 - val_acc: 0.7808\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4345 - acc: 0.7909 - val_loss: 0.4608 - val_acc: 0.7755\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4339 - acc: 0.7898 - val_loss: 0.4629 - val_acc: 0.7781\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4339 - acc: 0.7906 - val_loss: 0.4603 - val_acc: 0.7755\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4337 - acc: 0.7885 - val_loss: 0.4611 - val_acc: 0.7781\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4333 - acc: 0.7914 - val_loss: 0.4665 - val_acc: 0.7781\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4338 - acc: 0.7895 - val_loss: 0.4599 - val_acc: 0.7744\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4331 - acc: 0.7904 - val_loss: 0.4589 - val_acc: 0.7760\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4328 - acc: 0.7919 - val_loss: 0.4662 - val_acc: 0.7797\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4328 - acc: 0.7911 - val_loss: 0.4601 - val_acc: 0.7766\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4333 - acc: 0.7904 - val_loss: 0.4602 - val_acc: 0.7771\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4325 - acc: 0.7923 - val_loss: 0.4628 - val_acc: 0.7781\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4326 - acc: 0.7928 - val_loss: 0.4619 - val_acc: 0.7771\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4325 - acc: 0.7894 - val_loss: 0.4604 - val_acc: 0.7755\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4326 - acc: 0.7912 - val_loss: 0.4595 - val_acc: 0.7744\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4323 - acc: 0.7896 - val_loss: 0.4616 - val_acc: 0.7787\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4324 - acc: 0.7923 - val_loss: 0.4631 - val_acc: 0.7781\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4320 - acc: 0.7921 - val_loss: 0.4641 - val_acc: 0.7781\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 0.4598 - val_acc: 0.7750\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.4317 - acc: 0.7908 - val_loss: 0.4594 - val_acc: 0.7755\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4320 - acc: 0.7895 - val_loss: 0.4626 - val_acc: 0.7776\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.4317 - acc: 0.7931 - val_loss: 0.4603 - val_acc: 0.7771\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4315 - acc: 0.7925 - val_loss: 0.4612 - val_acc: 0.7771\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "# fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "# X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=7)\n",
    "X_ICA = ica.fit_transform(X_train)\n",
    "X_ICA_val = ica.transform(X_val)\n",
    "X_ICA_test = ica.transform(X_test)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = GaussianMixture(n_components=5)\n",
    "km = km.fit(X_ICA)\n",
    "X_train_kmeans = km.predict_proba(X_ICA)\n",
    "X_val_kmeans = km.predict_proba(X_ICA_val)\n",
    "X_test_kmeans = km.predict_proba(X_ICA_test)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "# X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "# X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_kmeans, y_train.values, epochs=60, validation_data=(X_val_kmeans, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7928496319663512\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_kmeans)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15216, 10)\n",
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 5s 308us/step - loss: 0.5120 - acc: 0.7604 - val_loss: 0.4559 - val_acc: 0.7760\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.4403 - acc: 0.7842 - val_loss: 0.4503 - val_acc: 0.7766\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.4347 - acc: 0.7870 - val_loss: 0.4478 - val_acc: 0.7792\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 167us/step - loss: 0.4291 - acc: 0.7909 - val_loss: 0.4415 - val_acc: 0.7802\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4239 - acc: 0.7953 - val_loss: 0.4365 - val_acc: 0.7823\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4192 - acc: 0.7980 - val_loss: 0.4343 - val_acc: 0.7839\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.4151 - acc: 0.8005 - val_loss: 0.4332 - val_acc: 0.7855\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.4130 - acc: 0.8042 - val_loss: 0.4306 - val_acc: 0.7813\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.4103 - acc: 0.8047 - val_loss: 0.4322 - val_acc: 0.7776\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.4086 - acc: 0.8070 - val_loss: 0.4288 - val_acc: 0.7823\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.4066 - acc: 0.8095 - val_loss: 0.4280 - val_acc: 0.7907\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 164us/step - loss: 0.4057 - acc: 0.8084 - val_loss: 0.4287 - val_acc: 0.7802\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4041 - acc: 0.8103 - val_loss: 0.4281 - val_acc: 0.7850\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.4029 - acc: 0.8114 - val_loss: 0.4223 - val_acc: 0.8002\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 167us/step - loss: 0.4020 - acc: 0.8136 - val_loss: 0.4211 - val_acc: 0.8018\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4002 - acc: 0.8150 - val_loss: 0.4203 - val_acc: 0.8044\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.3991 - acc: 0.8167 - val_loss: 0.4217 - val_acc: 0.8007\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.3979 - acc: 0.8162 - val_loss: 0.4193 - val_acc: 0.8023\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 224us/step - loss: 0.3971 - acc: 0.8179 - val_loss: 0.4183 - val_acc: 0.8039\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 3s 204us/step - loss: 0.3970 - acc: 0.8174 - val_loss: 0.4201 - val_acc: 0.8013\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.3961 - acc: 0.8185 - val_loss: 0.4186 - val_acc: 0.8023\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3951 - acc: 0.8185 - val_loss: 0.4212 - val_acc: 0.7976\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3948 - acc: 0.8190 - val_loss: 0.4244 - val_acc: 0.7913\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3947 - acc: 0.8193 - val_loss: 0.4183 - val_acc: 0.8044\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.3935 - acc: 0.8197 - val_loss: 0.4176 - val_acc: 0.8065\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 191us/step - loss: 0.3936 - acc: 0.8208 - val_loss: 0.4172 - val_acc: 0.8039\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3929 - acc: 0.8204 - val_loss: 0.4277 - val_acc: 0.7950\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3921 - acc: 0.8214 - val_loss: 0.4175 - val_acc: 0.8034\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3922 - acc: 0.8205 - val_loss: 0.4140 - val_acc: 0.8049\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3916 - acc: 0.8227 - val_loss: 0.4179 - val_acc: 0.7971\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3912 - acc: 0.8206 - val_loss: 0.4156 - val_acc: 0.8034\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3908 - acc: 0.8218 - val_loss: 0.4139 - val_acc: 0.8076\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3907 - acc: 0.8231 - val_loss: 0.4168 - val_acc: 0.8070\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3898 - acc: 0.8224 - val_loss: 0.4131 - val_acc: 0.8055\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3893 - acc: 0.8230 - val_loss: 0.4147 - val_acc: 0.8076\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3890 - acc: 0.8249 - val_loss: 0.4125 - val_acc: 0.8039\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.3889 - acc: 0.8223 - val_loss: 0.4115 - val_acc: 0.8097\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3883 - acc: 0.8215 - val_loss: 0.4146 - val_acc: 0.8086\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3880 - acc: 0.8240 - val_loss: 0.4110 - val_acc: 0.8060\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3872 - acc: 0.8229 - val_loss: 0.4101 - val_acc: 0.8076\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3874 - acc: 0.8247 - val_loss: 0.4229 - val_acc: 0.7902\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3870 - acc: 0.8243 - val_loss: 0.4147 - val_acc: 0.8060\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.3862 - acc: 0.8232 - val_loss: 0.4109 - val_acc: 0.8065\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3860 - acc: 0.8244 - val_loss: 0.4096 - val_acc: 0.8070\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3856 - acc: 0.8258 - val_loss: 0.4112 - val_acc: 0.8065\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3855 - acc: 0.8248 - val_loss: 0.4099 - val_acc: 0.8070\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 193us/step - loss: 0.3851 - acc: 0.8263 - val_loss: 0.4116 - val_acc: 0.8060\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 196us/step - loss: 0.3845 - acc: 0.8244 - val_loss: 0.4105 - val_acc: 0.8065\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.3844 - acc: 0.8258 - val_loss: 0.4098 - val_acc: 0.8055\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3840 - acc: 0.8249 - val_loss: 0.4088 - val_acc: 0.8076\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3843 - acc: 0.8239 - val_loss: 0.4104 - val_acc: 0.8081\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3837 - acc: 0.8253 - val_loss: 0.4097 - val_acc: 0.8060\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3840 - acc: 0.8253 - val_loss: 0.4067 - val_acc: 0.8123\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3831 - acc: 0.8266 - val_loss: 0.4145 - val_acc: 0.8060\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3835 - acc: 0.8244 - val_loss: 0.4071 - val_acc: 0.8076\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3830 - acc: 0.8256 - val_loss: 0.4059 - val_acc: 0.8070\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3827 - acc: 0.8248 - val_loss: 0.4126 - val_acc: 0.8060\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3821 - acc: 0.8255 - val_loss: 0.4078 - val_acc: 0.8060\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3825 - acc: 0.8259 - val_loss: 0.4080 - val_acc: 0.8091\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3823 - acc: 0.8245 - val_loss: 0.4110 - val_acc: 0.8070\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(10,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "# fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "# X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=5)\n",
    "X_ICA = ica.fit_transform(X_train)\n",
    "X_ICA_val = ica.transform(X_val)\n",
    "X_ICA_test = ica.transform(X_test)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = GaussianMixture(n_components=5)\n",
    "km = km.fit(X_train)\n",
    "X_train_kmeans = km.predict_proba(X_train)\n",
    "X_val_kmeans = km.predict_proba(X_val)\n",
    "X_test_kmeans = km.predict_proba(X_test)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "# X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "# X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "X_train_comb = np.concatenate((X_ICA, X_train_kmeans), axis = 1)\n",
    "X_val_comb = np.concatenate((X_ICA_val, X_val_kmeans), axis = 1)\n",
    "X_test_comb = np.concatenate((X_ICA_test, X_test_kmeans), axis = 1)\n",
    "print(X_train_comb.shape)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_comb, y_train.values, epochs=60, validation_data=(X_val_comb, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8159831756046267\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_comb)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chunlok Lo\\AppData\\Local\\conda\\conda\\envs\\magiclearning\\lib\\site-packages\\sklearn\\decomposition\\fastica_.py:121: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15216, 13)\n",
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 5s 339us/step - loss: 0.5472 - acc: 0.7371 - val_loss: 0.4912 - val_acc: 0.7687\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4642 - acc: 0.7766 - val_loss: 0.4853 - val_acc: 0.7708\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4545 - acc: 0.7777 - val_loss: 0.4754 - val_acc: 0.7744\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.4431 - acc: 0.7857 - val_loss: 0.4672 - val_acc: 0.7992\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4294 - acc: 0.8047 - val_loss: 0.4509 - val_acc: 0.7960\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4158 - acc: 0.8160 - val_loss: 0.4418 - val_acc: 0.8002\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4058 - acc: 0.8225 - val_loss: 0.4328 - val_acc: 0.8018\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3993 - acc: 0.8247 - val_loss: 0.4314 - val_acc: 0.7997\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.3949 - acc: 0.8265 - val_loss: 0.4268 - val_acc: 0.8065\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3910 - acc: 0.8282 - val_loss: 0.4232 - val_acc: 0.8097\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3884 - acc: 0.8303 - val_loss: 0.4235 - val_acc: 0.8060\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.3853 - acc: 0.8313 - val_loss: 0.4202 - val_acc: 0.8144\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3836 - acc: 0.8332 - val_loss: 0.4137 - val_acc: 0.8128\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3811 - acc: 0.8348 - val_loss: 0.4153 - val_acc: 0.8155\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3790 - acc: 0.8368 - val_loss: 0.4151 - val_acc: 0.8155\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3776 - acc: 0.8356 - val_loss: 0.4112 - val_acc: 0.8176\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3754 - acc: 0.8392 - val_loss: 0.4074 - val_acc: 0.8223\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.3740 - acc: 0.8396 - val_loss: 0.4052 - val_acc: 0.8233\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.3729 - acc: 0.8396 - val_loss: 0.4047 - val_acc: 0.8239\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 3s 194us/step - loss: 0.3718 - acc: 0.8422 - val_loss: 0.4036 - val_acc: 0.8239\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3719 - acc: 0.8417 - val_loss: 0.4042 - val_acc: 0.8260\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.3707 - acc: 0.8431 - val_loss: 0.4104 - val_acc: 0.8212\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 3s 192us/step - loss: 0.3699 - acc: 0.8440 - val_loss: 0.4011 - val_acc: 0.8265\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.3695 - acc: 0.8435 - val_loss: 0.4017 - val_acc: 0.8275\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3679 - acc: 0.8440 - val_loss: 0.4017 - val_acc: 0.8286\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.3678 - acc: 0.8446 - val_loss: 0.4011 - val_acc: 0.8265\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3678 - acc: 0.8452 - val_loss: 0.4006 - val_acc: 0.8302\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.3666 - acc: 0.8448 - val_loss: 0.3989 - val_acc: 0.8297\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3667 - acc: 0.8434 - val_loss: 0.3991 - val_acc: 0.8307\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.3660 - acc: 0.8448 - val_loss: 0.3981 - val_acc: 0.8318\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.3653 - acc: 0.8444 - val_loss: 0.3979 - val_acc: 0.8339\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.3645 - acc: 0.8466 - val_loss: 0.3987 - val_acc: 0.8323\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 196us/step - loss: 0.3651 - acc: 0.8452 - val_loss: 0.3987 - val_acc: 0.8328\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 191us/step - loss: 0.3641 - acc: 0.8458 - val_loss: 0.3971 - val_acc: 0.8323\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3641 - acc: 0.8456 - val_loss: 0.3990 - val_acc: 0.8275\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3628 - acc: 0.8461 - val_loss: 0.3951 - val_acc: 0.8312\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3625 - acc: 0.8465 - val_loss: 0.3948 - val_acc: 0.8386\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3624 - acc: 0.8464 - val_loss: 0.3924 - val_acc: 0.8333\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 195us/step - loss: 0.3620 - acc: 0.8471 - val_loss: 0.3926 - val_acc: 0.8360\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 191us/step - loss: 0.3616 - acc: 0.8486 - val_loss: 0.3947 - val_acc: 0.8312\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3613 - acc: 0.8456 - val_loss: 0.3915 - val_acc: 0.8349\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3605 - acc: 0.8481 - val_loss: 0.3988 - val_acc: 0.8249\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3591 - acc: 0.8475 - val_loss: 0.3952 - val_acc: 0.8318\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 186us/step - loss: 0.3609 - acc: 0.8460 - val_loss: 0.3939 - val_acc: 0.8318\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 194us/step - loss: 0.3589 - acc: 0.8484 - val_loss: 0.3898 - val_acc: 0.8323\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3585 - acc: 0.8496 - val_loss: 0.3893 - val_acc: 0.8318\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.3583 - acc: 0.8480 - val_loss: 0.3881 - val_acc: 0.8370\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.3583 - acc: 0.8487 - val_loss: 0.3874 - val_acc: 0.8365\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3571 - acc: 0.8493 - val_loss: 0.3903 - val_acc: 0.8318\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3578 - acc: 0.8481 - val_loss: 0.3899 - val_acc: 0.8344\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3565 - acc: 0.8479 - val_loss: 0.3867 - val_acc: 0.8391\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3566 - acc: 0.8489 - val_loss: 0.3880 - val_acc: 0.8318\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3565 - acc: 0.8494 - val_loss: 0.3923 - val_acc: 0.8323\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3549 - acc: 0.8496 - val_loss: 0.3876 - val_acc: 0.8344\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3563 - acc: 0.8488 - val_loss: 0.3846 - val_acc: 0.8333\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3549 - acc: 0.8503 - val_loss: 0.3889 - val_acc: 0.8370\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.3542 - acc: 0.8506 - val_loss: 0.3867 - val_acc: 0.8339\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3540 - acc: 0.8506 - val_loss: 0.3862 - val_acc: 0.8386\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3541 - acc: 0.8503 - val_loss: 0.3846 - val_acc: 0.8375\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3542 - acc: 0.8504 - val_loss: 0.3874 - val_acc: 0.8339\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(13,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "# fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "# X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=10)\n",
    "X_ICA = ica.fit_transform(X_train)\n",
    "X_ICA_val = ica.transform(X_val)\n",
    "X_ICA_test = ica.transform(X_test)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = GaussianMixture(n_components=3)\n",
    "km = km.fit(X_train)\n",
    "X_train_kmeans = km.predict_proba(X_train)\n",
    "X_val_kmeans = km.predict_proba(X_val)\n",
    "X_test_kmeans = km.predict_proba(X_test)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "# X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "# X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "X_train_comb = np.concatenate((X_ICA, X_train_kmeans), axis = 1)\n",
    "X_val_comb = np.concatenate((X_ICA_val, X_val_kmeans), axis = 1)\n",
    "X_test_comb = np.concatenate((X_ICA_test, X_test_kmeans), axis = 1)\n",
    "print(X_train_comb.shape)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_comb, y_train.values, epochs=60, validation_data=(X_val_comb, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8585699263932702\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_comb)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15216, 10)\n",
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 5s 355us/step - loss: 0.5623 - acc: 0.7416 - val_loss: 0.4906 - val_acc: 0.7676\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4642 - acc: 0.7758 - val_loss: 0.4855 - val_acc: 0.7708\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.4573 - acc: 0.7764 - val_loss: 0.4776 - val_acc: 0.7718\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 191us/step - loss: 0.4495 - acc: 0.7795 - val_loss: 0.4747 - val_acc: 0.7808\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4421 - acc: 0.7895 - val_loss: 0.4651 - val_acc: 0.7744\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4336 - acc: 0.7978 - val_loss: 0.4555 - val_acc: 0.7839\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 194us/step - loss: 0.4265 - acc: 0.8021 - val_loss: 0.4506 - val_acc: 0.7855\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 4s 230us/step - loss: 0.4211 - acc: 0.8038 - val_loss: 0.4453 - val_acc: 0.7871\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 213us/step - loss: 0.4171 - acc: 0.8034 - val_loss: 0.4467 - val_acc: 0.7923\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4143 - acc: 0.8059 - val_loss: 0.4413 - val_acc: 0.7939\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4130 - acc: 0.8072 - val_loss: 0.4410 - val_acc: 0.7944\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4108 - acc: 0.8099 - val_loss: 0.4398 - val_acc: 0.7934\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 195us/step - loss: 0.4095 - acc: 0.8086 - val_loss: 0.4372 - val_acc: 0.7965\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 209us/step - loss: 0.4077 - acc: 0.8107 - val_loss: 0.4346 - val_acc: 0.7992\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 195us/step - loss: 0.4065 - acc: 0.8105 - val_loss: 0.4331 - val_acc: 0.7997\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4047 - acc: 0.8139 - val_loss: 0.4351 - val_acc: 0.8013\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4040 - acc: 0.8141 - val_loss: 0.4361 - val_acc: 0.8039\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4035 - acc: 0.8141 - val_loss: 0.4302 - val_acc: 0.8055\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4020 - acc: 0.8158 - val_loss: 0.4326 - val_acc: 0.8034\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4013 - acc: 0.8179 - val_loss: 0.4306 - val_acc: 0.8060\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4006 - acc: 0.8185 - val_loss: 0.4281 - val_acc: 0.8060\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3996 - acc: 0.8189 - val_loss: 0.4283 - val_acc: 0.8113\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3986 - acc: 0.8206 - val_loss: 0.4278 - val_acc: 0.8049\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.3986 - acc: 0.8180 - val_loss: 0.4293 - val_acc: 0.8076\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.3985 - acc: 0.8196 - val_loss: 0.4268 - val_acc: 0.8097\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 197us/step - loss: 0.3970 - acc: 0.8201 - val_loss: 0.4336 - val_acc: 0.8060\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.3972 - acc: 0.8231 - val_loss: 0.4255 - val_acc: 0.8102\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.3970 - acc: 0.8216 - val_loss: 0.4256 - val_acc: 0.8091\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 191us/step - loss: 0.3961 - acc: 0.8223 - val_loss: 0.4243 - val_acc: 0.8081\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.3954 - acc: 0.8243 - val_loss: 0.4255 - val_acc: 0.8139\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3954 - acc: 0.8231 - val_loss: 0.4246 - val_acc: 0.8076\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.3954 - acc: 0.8238 - val_loss: 0.4241 - val_acc: 0.8128\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.3950 - acc: 0.8238 - val_loss: 0.4239 - val_acc: 0.8107\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.3948 - acc: 0.8255 - val_loss: 0.4233 - val_acc: 0.8107\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3939 - acc: 0.8243 - val_loss: 0.4334 - val_acc: 0.8044\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3936 - acc: 0.8233 - val_loss: 0.4220 - val_acc: 0.8118\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 195us/step - loss: 0.3935 - acc: 0.8260 - val_loss: 0.4213 - val_acc: 0.8139\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.3926 - acc: 0.8272 - val_loss: 0.4240 - val_acc: 0.8176\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3930 - acc: 0.8251 - val_loss: 0.4201 - val_acc: 0.8155\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3925 - acc: 0.8252 - val_loss: 0.4233 - val_acc: 0.8128\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 221us/step - loss: 0.3921 - acc: 0.8270 - val_loss: 0.4201 - val_acc: 0.8123\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 198us/step - loss: 0.3918 - acc: 0.8270 - val_loss: 0.4203 - val_acc: 0.8107\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.3918 - acc: 0.8258 - val_loss: 0.4193 - val_acc: 0.8139\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3911 - acc: 0.8253 - val_loss: 0.4225 - val_acc: 0.8186\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.3911 - acc: 0.8272 - val_loss: 0.4196 - val_acc: 0.8197\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3904 - acc: 0.8275 - val_loss: 0.4207 - val_acc: 0.8113\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3902 - acc: 0.8270 - val_loss: 0.4210 - val_acc: 0.8160\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.3897 - acc: 0.8271 - val_loss: 0.4205 - val_acc: 0.8176\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3904 - acc: 0.8258 - val_loss: 0.4187 - val_acc: 0.8149\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3893 - acc: 0.8272 - val_loss: 0.4178 - val_acc: 0.8128\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3894 - acc: 0.8285 - val_loss: 0.4177 - val_acc: 0.8118\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 196us/step - loss: 0.3886 - acc: 0.8275 - val_loss: 0.4196 - val_acc: 0.8160\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 196us/step - loss: 0.3881 - acc: 0.8297 - val_loss: 0.4189 - val_acc: 0.8186\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3882 - acc: 0.8283 - val_loss: 0.4162 - val_acc: 0.8144\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3881 - acc: 0.8285 - val_loss: 0.4187 - val_acc: 0.8155\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.3876 - acc: 0.8298 - val_loss: 0.4172 - val_acc: 0.8197\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.3872 - acc: 0.8304 - val_loss: 0.4153 - val_acc: 0.8149\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 199us/step - loss: 0.3865 - acc: 0.8302 - val_loss: 0.4153 - val_acc: 0.8186\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3865 - acc: 0.8307 - val_loss: 0.4158 - val_acc: 0.8197\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.3865 - acc: 0.8309 - val_loss: 0.4146 - val_acc: 0.8139\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(10,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "# fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "# X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=7)\n",
    "X_ICA = ica.fit_transform(X_train)\n",
    "X_ICA_val = ica.transform(X_val)\n",
    "X_ICA_test = ica.transform(X_test)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = GaussianMixture(n_components=3)\n",
    "km = km.fit(X_train)\n",
    "X_train_kmeans = km.predict_proba(X_train)\n",
    "X_val_kmeans = km.predict_proba(X_val)\n",
    "X_test_kmeans = km.predict_proba(X_test)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "# X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "# X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "X_train_comb = np.concatenate((X_ICA, X_train_kmeans), axis = 1)\n",
    "X_val_comb = np.concatenate((X_ICA_val, X_val_kmeans), axis = 1)\n",
    "X_test_comb = np.concatenate((X_ICA_test, X_test_kmeans), axis = 1)\n",
    "print(X_train_comb.shape)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_comb, y_train.values, epochs=60, validation_data=(X_val_comb, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = model.predict(X_test_comb)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.59834648e+03 1.13544126e+03 2.16394402e+02 1.13813546e+01\n",
      " 6.31846032e-01 5.03203921e+02 6.01728740e+02 2.05423661e-01\n",
      " 4.19801232e+03 6.45388515e+01]\n",
      "[8 0 1 6 5]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prnitsdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-795311320729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfvalue_selector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvalue_selector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mprnitsdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mX_ICA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfvalue_selector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prnitsdf' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(7,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "fvalue_selector = SelectKBest(f_classif, k=5)\n",
    "fvalue_selector.fit(X_train, y_train)\n",
    "# X_kbest = fvalue_selector.fit_transform(X, y)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "\n",
    "print(fvalue_selector.scores_)\n",
    "print(fvalue_selector.scores_.argsort()[-5:][::-1])\n",
    "plt.bar(range(10), fvalue_selector.scores_)\n",
    "prnitsdf\n",
    "\n",
    "X_ICA = fvalue_selector.transform(X_train)\n",
    "X_ICA_val = fvalue_selector.transform(X_val)\n",
    "X_ICA_test = fvalue_selector.transform(X_test)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = GaussianMixture(n_components=2)\n",
    "km = km.fit(X_train)\n",
    "X_train_kmeans = km.predict_proba(X_train)\n",
    "X_val_kmeans = km.predict_proba(X_val)\n",
    "X_test_kmeans = km.predict_proba(X_test)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "# X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "# X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "X_train_comb = np.concatenate((X_ICA, X_train_kmeans), axis = 1)\n",
    "X_val_comb = np.concatenate((X_ICA_val, X_val_kmeans), axis = 1)\n",
    "X_test_comb = np.concatenate((X_ICA_test, X_test_kmeans), axis = 1)\n",
    "print(X_train_comb.shape)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_comb, y_train.values, epochs=60, validation_data=(X_val_comb, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825446898002103\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_comb)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15216, 9)\n",
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 5s 354us/step - loss: 0.5589 - acc: 0.7351 - val_loss: 0.4641 - val_acc: 0.7823\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4469 - acc: 0.7827 - val_loss: 0.4613 - val_acc: 0.7839\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4435 - acc: 0.7833 - val_loss: 0.4597 - val_acc: 0.7823\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4403 - acc: 0.7842 - val_loss: 0.4546 - val_acc: 0.7855\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.4368 - acc: 0.7854 - val_loss: 0.4513 - val_acc: 0.7860\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4321 - acc: 0.7929 - val_loss: 0.4474 - val_acc: 0.7871\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4278 - acc: 0.7978 - val_loss: 0.4461 - val_acc: 0.7939\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4235 - acc: 0.8039 - val_loss: 0.4397 - val_acc: 0.7918\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.4195 - acc: 0.8068 - val_loss: 0.4382 - val_acc: 0.7944\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4164 - acc: 0.8054 - val_loss: 0.4349 - val_acc: 0.7944\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.4143 - acc: 0.8057 - val_loss: 0.4326 - val_acc: 0.7944\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4123 - acc: 0.8080 - val_loss: 0.4344 - val_acc: 0.7971\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4105 - acc: 0.8091 - val_loss: 0.4324 - val_acc: 0.7955\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4092 - acc: 0.8090 - val_loss: 0.4303 - val_acc: 0.7971\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.4081 - acc: 0.8099 - val_loss: 0.4289 - val_acc: 0.7976\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4073 - acc: 0.8109 - val_loss: 0.4279 - val_acc: 0.7981\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 200us/step - loss: 0.4062 - acc: 0.8120 - val_loss: 0.4279 - val_acc: 0.8007\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 192us/step - loss: 0.4053 - acc: 0.8115 - val_loss: 0.4270 - val_acc: 0.7986\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 203us/step - loss: 0.4046 - acc: 0.8100 - val_loss: 0.4312 - val_acc: 0.8013\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 4s 258us/step - loss: 0.4036 - acc: 0.8126 - val_loss: 0.4252 - val_acc: 0.8002\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 4s 264us/step - loss: 0.4034 - acc: 0.8139 - val_loss: 0.4250 - val_acc: 0.8002\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 212us/step - loss: 0.4023 - acc: 0.8134 - val_loss: 0.4250 - val_acc: 0.8034\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 4s 243us/step - loss: 0.4013 - acc: 0.8146 - val_loss: 0.4239 - val_acc: 0.8007\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 218us/step - loss: 0.4002 - acc: 0.8169 - val_loss: 0.4247 - val_acc: 0.8023\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 198us/step - loss: 0.4003 - acc: 0.8163 - val_loss: 0.4242 - val_acc: 0.8018\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 211us/step - loss: 0.3994 - acc: 0.8166 - val_loss: 0.4236 - val_acc: 0.8034\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 208us/step - loss: 0.3996 - acc: 0.8182 - val_loss: 0.4229 - val_acc: 0.8039\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.3989 - acc: 0.8182 - val_loss: 0.4217 - val_acc: 0.7997\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.3984 - acc: 0.8169 - val_loss: 0.4223 - val_acc: 0.8049\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3985 - acc: 0.8163 - val_loss: 0.4223 - val_acc: 0.8034\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3976 - acc: 0.8185 - val_loss: 0.4207 - val_acc: 0.8018\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.3978 - acc: 0.8185 - val_loss: 0.4213 - val_acc: 0.8065\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.3975 - acc: 0.8186 - val_loss: 0.4258 - val_acc: 0.8039\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.3970 - acc: 0.8186 - val_loss: 0.4213 - val_acc: 0.8013\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 213us/step - loss: 0.3968 - acc: 0.8178 - val_loss: 0.4213 - val_acc: 0.8028\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 205us/step - loss: 0.3964 - acc: 0.8196 - val_loss: 0.4220 - val_acc: 0.8055\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 204us/step - loss: 0.3968 - acc: 0.8181 - val_loss: 0.4207 - val_acc: 0.8060\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 191us/step - loss: 0.3959 - acc: 0.8189 - val_loss: 0.4197 - val_acc: 0.8076\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 193us/step - loss: 0.3965 - acc: 0.8198 - val_loss: 0.4197 - val_acc: 0.8065\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.3959 - acc: 0.8175 - val_loss: 0.4211 - val_acc: 0.8065\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 198us/step - loss: 0.3955 - acc: 0.8203 - val_loss: 0.4202 - val_acc: 0.8060\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3952 - acc: 0.8190 - val_loss: 0.4233 - val_acc: 0.7992\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 199us/step - loss: 0.3949 - acc: 0.8195 - val_loss: 0.4189 - val_acc: 0.8065\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.3947 - acc: 0.8188 - val_loss: 0.4180 - val_acc: 0.8076\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 200us/step - loss: 0.3948 - acc: 0.8190 - val_loss: 0.4212 - val_acc: 0.8049\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 219us/step - loss: 0.3946 - acc: 0.8202 - val_loss: 0.4176 - val_acc: 0.8091\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 209us/step - loss: 0.3941 - acc: 0.8191 - val_loss: 0.4242 - val_acc: 0.8034\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.3944 - acc: 0.8199 - val_loss: 0.4184 - val_acc: 0.8081\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 194us/step - loss: 0.3939 - acc: 0.8190 - val_loss: 0.4205 - val_acc: 0.8065\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 217us/step - loss: 0.3937 - acc: 0.8207 - val_loss: 0.4223 - val_acc: 0.8060\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 199us/step - loss: 0.3935 - acc: 0.8210 - val_loss: 0.4196 - val_acc: 0.8039\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 212us/step - loss: 0.3927 - acc: 0.8207 - val_loss: 0.4184 - val_acc: 0.8049\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.3930 - acc: 0.8201 - val_loss: 0.4187 - val_acc: 0.8107\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3928 - acc: 0.8216 - val_loss: 0.4175 - val_acc: 0.8097\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 191us/step - loss: 0.3929 - acc: 0.8211 - val_loss: 0.4171 - val_acc: 0.8107\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3926 - acc: 0.8202 - val_loss: 0.4168 - val_acc: 0.8113\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.3922 - acc: 0.8213 - val_loss: 0.4190 - val_acc: 0.8118\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 209us/step - loss: 0.3919 - acc: 0.8215 - val_loss: 0.4192 - val_acc: 0.8023\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.3921 - acc: 0.8220 - val_loss: 0.4166 - val_acc: 0.8107\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 180us/step - loss: 0.3917 - acc: 0.8224 - val_loss: 0.4152 - val_acc: 0.8086\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(9,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=5)\n",
    "X_ICA = ica.fit_transform(X_train)\n",
    "X_ICA_val = ica.transform(X_val)\n",
    "X_ICA_test = ica.transform(X_test)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = GaussianMixture(n_components=4)\n",
    "km = km.fit(X_train)\n",
    "X_train_kmeans = km.predict_proba(X_train)\n",
    "X_val_kmeans = km.predict_proba(X_val)\n",
    "X_test_kmeans = km.predict_proba(X_test)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "# X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "# X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "X_train_comb = np.concatenate((X_ICA, X_train_kmeans), axis = 1)\n",
    "X_val_comb = np.concatenate((X_ICA_val, X_val_kmeans), axis = 1)\n",
    "X_test_comb = np.concatenate((X_ICA_test, X_test_kmeans), axis = 1)\n",
    "print(X_train_comb.shape)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_comb, y_train.values, epochs=60, validation_data=(X_val_comb, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8191377497371188\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_comb)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15216, 9)\n",
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 9s 563us/step - loss: 0.6205 - acc: 0.6917 - val_loss: 0.6049 - val_acc: 0.6887\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.5905 - acc: 0.6927 - val_loss: 0.5894 - val_acc: 0.6887\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.5664 - acc: 0.7007 - val_loss: 0.5600 - val_acc: 0.7303\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 3s 171us/step - loss: 0.5252 - acc: 0.7428 - val_loss: 0.5169 - val_acc: 0.7592\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 3s 173us/step - loss: 0.4882 - acc: 0.7750 - val_loss: 0.4964 - val_acc: 0.7681\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 3s 175us/step - loss: 0.4734 - acc: 0.7829 - val_loss: 0.4890 - val_acc: 0.7718\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 197us/step - loss: 0.4661 - acc: 0.7899 - val_loss: 0.4910 - val_acc: 0.7792\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 3s 209us/step - loss: 0.4637 - acc: 0.7910 - val_loss: 0.4849 - val_acc: 0.7739\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 3s 182us/step - loss: 0.4603 - acc: 0.7942 - val_loss: 0.4818 - val_acc: 0.7776\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 4s 231us/step - loss: 0.4584 - acc: 0.7953 - val_loss: 0.4843 - val_acc: 0.7834\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 194us/step - loss: 0.4568 - acc: 0.7963 - val_loss: 0.4838 - val_acc: 0.7739\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 204us/step - loss: 0.4552 - acc: 0.7978 - val_loss: 0.4821 - val_acc: 0.7802\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 207us/step - loss: 0.4545 - acc: 0.7993 - val_loss: 0.4758 - val_acc: 0.7792\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4532 - acc: 0.7987 - val_loss: 0.4803 - val_acc: 0.7797\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4521 - acc: 0.8006 - val_loss: 0.4770 - val_acc: 0.7865\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 3s 174us/step - loss: 0.4518 - acc: 0.8021 - val_loss: 0.4745 - val_acc: 0.7818\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4503 - acc: 0.8033 - val_loss: 0.4733 - val_acc: 0.7876\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4493 - acc: 0.8034 - val_loss: 0.4722 - val_acc: 0.7834\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4506 - acc: 0.8017 - val_loss: 0.4765 - val_acc: 0.7844\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4503 - acc: 0.8042 - val_loss: 0.4692 - val_acc: 0.7855\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4487 - acc: 0.8044 - val_loss: 0.4719 - val_acc: 0.7850\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4481 - acc: 0.8041 - val_loss: 0.4729 - val_acc: 0.7818\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4487 - acc: 0.8048 - val_loss: 0.4709 - val_acc: 0.7897\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 3s 202us/step - loss: 0.4481 - acc: 0.8028 - val_loss: 0.4675 - val_acc: 0.7860\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4479 - acc: 0.8036 - val_loss: 0.4674 - val_acc: 0.7886\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4480 - acc: 0.8042 - val_loss: 0.4737 - val_acc: 0.7918\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4476 - acc: 0.8050 - val_loss: 0.4720 - val_acc: 0.7808\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 181us/step - loss: 0.4468 - acc: 0.8056 - val_loss: 0.4825 - val_acc: 0.7876\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4472 - acc: 0.8061 - val_loss: 0.4677 - val_acc: 0.7871\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4469 - acc: 0.8040 - val_loss: 0.4657 - val_acc: 0.7871\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4468 - acc: 0.8045 - val_loss: 0.4693 - val_acc: 0.7871\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 3s 190us/step - loss: 0.4465 - acc: 0.8053 - val_loss: 0.4655 - val_acc: 0.7871\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 3s 192us/step - loss: 0.4464 - acc: 0.8051 - val_loss: 0.4669 - val_acc: 0.7865\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 3s 189us/step - loss: 0.4453 - acc: 0.8076 - val_loss: 0.4678 - val_acc: 0.7876\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 3s 204us/step - loss: 0.4448 - acc: 0.8064 - val_loss: 0.4717 - val_acc: 0.7918\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 3s 194us/step - loss: 0.4451 - acc: 0.8045 - val_loss: 0.4683 - val_acc: 0.7839\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 3s 194us/step - loss: 0.4450 - acc: 0.8061 - val_loss: 0.4675 - val_acc: 0.7871\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 3s 193us/step - loss: 0.4445 - acc: 0.8068 - val_loss: 0.4713 - val_acc: 0.7913\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 3s 187us/step - loss: 0.4440 - acc: 0.8052 - val_loss: 0.4675 - val_acc: 0.7892\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 3s 199us/step - loss: 0.4444 - acc: 0.8040 - val_loss: 0.4664 - val_acc: 0.7797\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 3s 199us/step - loss: 0.4431 - acc: 0.8070 - val_loss: 0.4641 - val_acc: 0.7876\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 3s 205us/step - loss: 0.4427 - acc: 0.8067 - val_loss: 0.4628 - val_acc: 0.7881\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 3s 218us/step - loss: 0.4420 - acc: 0.8067 - val_loss: 0.4623 - val_acc: 0.7886\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 3s 205us/step - loss: 0.4414 - acc: 0.8073 - val_loss: 0.4630 - val_acc: 0.7876\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 3s 193us/step - loss: 0.4408 - acc: 0.8078 - val_loss: 0.4595 - val_acc: 0.7865\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 3s 192us/step - loss: 0.4406 - acc: 0.8080 - val_loss: 0.4592 - val_acc: 0.7886\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 3s 204us/step - loss: 0.4396 - acc: 0.8076 - val_loss: 0.4586 - val_acc: 0.7871\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 3s 214us/step - loss: 0.4395 - acc: 0.8068 - val_loss: 0.4597 - val_acc: 0.7928\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4390 - acc: 0.8076 - val_loss: 0.4598 - val_acc: 0.7934\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 177us/step - loss: 0.4394 - acc: 0.8067 - val_loss: 0.4598 - val_acc: 0.7839\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 3s 184us/step - loss: 0.4380 - acc: 0.8079 - val_loss: 0.4576 - val_acc: 0.7907\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 3s 218us/step - loss: 0.4368 - acc: 0.8066 - val_loss: 0.4545 - val_acc: 0.7907\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 3s 225us/step - loss: 0.4360 - acc: 0.8076 - val_loss: 0.4515 - val_acc: 0.7897\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 3s 196us/step - loss: 0.4348 - acc: 0.8076 - val_loss: 0.4530 - val_acc: 0.7871\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 3s 188us/step - loss: 0.4338 - acc: 0.8074 - val_loss: 0.4509 - val_acc: 0.7897\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 200us/step - loss: 0.4337 - acc: 0.8080 - val_loss: 0.4518 - val_acc: 0.7892\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 3s 199us/step - loss: 0.4330 - acc: 0.8085 - val_loss: 0.4548 - val_acc: 0.7918\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 206us/step - loss: 0.4330 - acc: 0.8089 - val_loss: 0.4510 - val_acc: 0.7897\n",
      "Epoch 59/60\n",
      "15216/15216 [==============================] - 3s 195us/step - loss: 0.4323 - acc: 0.8093 - val_loss: 0.4518 - val_acc: 0.7923\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 3s 183us/step - loss: 0.4318 - acc: 0.8091 - val_loss: 0.4504 - val_acc: 0.7928\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(9,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=5)\n",
    "X_ICA = ica.fit_transform(X_train)\n",
    "X_ICA_val = ica.transform(X_val)\n",
    "X_ICA_test = ica.transform(X_test)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = KMeans(n_clusters=4)\n",
    "km = km.fit(X_train)\n",
    "X_train_kmeans = km.predict(X_train)\n",
    "X_val_kmeans = km.predict(X_val)\n",
    "X_test_kmeans = km.predict(X_test)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "X_train_comb = np.concatenate((X_ICA, X_train_kmeans), axis = 1)\n",
    "X_val_comb = np.concatenate((X_ICA_val, X_val_kmeans), axis = 1)\n",
    "X_test_comb = np.concatenate((X_ICA_test, X_test_kmeans), axis = 1)\n",
    "print(X_train_comb.shape)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_comb, y_train.values, epochs=60, validation_data=(X_val_comb, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807570977917981\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_comb)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING F_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15216 samples, validate on 1902 samples\n",
      "Epoch 1/60\n",
      "15216/15216 [==============================] - 3s 178us/step - loss: 0.4920 - acc: 0.7815 - val_loss: 0.4173 - val_acc: 0.8081\n",
      "Epoch 2/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4368 - acc: 0.8019 - val_loss: 0.4157 - val_acc: 0.8081\n",
      "Epoch 3/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.4361 - acc: 0.8022 - val_loss: 0.4144 - val_acc: 0.8102\n",
      "Epoch 4/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.4355 - acc: 0.8038 - val_loss: 0.4137 - val_acc: 0.8128\n",
      "Epoch 5/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4355 - acc: 0.8035 - val_loss: 0.4132 - val_acc: 0.8091\n",
      "Epoch 6/60\n",
      "15216/15216 [==============================] - 2s 145us/step - loss: 0.4348 - acc: 0.8041 - val_loss: 0.4127 - val_acc: 0.8107\n",
      "Epoch 7/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.4347 - acc: 0.8038 - val_loss: 0.4133 - val_acc: 0.8155\n",
      "Epoch 8/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.4343 - acc: 0.8034 - val_loss: 0.4129 - val_acc: 0.8160\n",
      "Epoch 9/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.4340 - acc: 0.8029 - val_loss: 0.4131 - val_acc: 0.8165\n",
      "Epoch 10/60\n",
      "15216/15216 [==============================] - 2s 146us/step - loss: 0.4335 - acc: 0.8039 - val_loss: 0.4110 - val_acc: 0.8134\n",
      "Epoch 11/60\n",
      "15216/15216 [==============================] - 3s 168us/step - loss: 0.4334 - acc: 0.8053 - val_loss: 0.4120 - val_acc: 0.8102\n",
      "Epoch 12/60\n",
      "15216/15216 [==============================] - 3s 176us/step - loss: 0.4334 - acc: 0.8055 - val_loss: 0.4115 - val_acc: 0.8160\n",
      "Epoch 13/60\n",
      "15216/15216 [==============================] - 3s 172us/step - loss: 0.4329 - acc: 0.8045 - val_loss: 0.4111 - val_acc: 0.8170\n",
      "Epoch 14/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4330 - acc: 0.8052 - val_loss: 0.4103 - val_acc: 0.8102\n",
      "Epoch 15/60\n",
      "15216/15216 [==============================] - 2s 161us/step - loss: 0.4328 - acc: 0.8047 - val_loss: 0.4105 - val_acc: 0.8165\n",
      "Epoch 16/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.4321 - acc: 0.8053 - val_loss: 0.4106 - val_acc: 0.8149\n",
      "Epoch 17/60\n",
      "15216/15216 [==============================] - 3s 170us/step - loss: 0.4324 - acc: 0.8058 - val_loss: 0.4096 - val_acc: 0.8155\n",
      "Epoch 18/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.4322 - acc: 0.8059 - val_loss: 0.4107 - val_acc: 0.8165\n",
      "Epoch 19/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4319 - acc: 0.8048 - val_loss: 0.4105 - val_acc: 0.8113\n",
      "Epoch 20/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4321 - acc: 0.8050 - val_loss: 0.4101 - val_acc: 0.8144\n",
      "Epoch 21/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4318 - acc: 0.8063 - val_loss: 0.4089 - val_acc: 0.8144\n",
      "Epoch 22/60\n",
      "15216/15216 [==============================] - 2s 149us/step - loss: 0.4316 - acc: 0.8055 - val_loss: 0.4099 - val_acc: 0.8160\n",
      "Epoch 23/60\n",
      "15216/15216 [==============================] - 2s 158us/step - loss: 0.4317 - acc: 0.8059 - val_loss: 0.4090 - val_acc: 0.8149\n",
      "Epoch 24/60\n",
      "15216/15216 [==============================] - 2s 164us/step - loss: 0.4316 - acc: 0.8064 - val_loss: 0.4089 - val_acc: 0.8181\n",
      "Epoch 25/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4313 - acc: 0.8047 - val_loss: 0.4080 - val_acc: 0.8181\n",
      "Epoch 26/60\n",
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.4309 - acc: 0.8057 - val_loss: 0.4087 - val_acc: 0.8144\n",
      "Epoch 27/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.4311 - acc: 0.8061 - val_loss: 0.4078 - val_acc: 0.8165\n",
      "Epoch 28/60\n",
      "15216/15216 [==============================] - 3s 165us/step - loss: 0.4307 - acc: 0.8051 - val_loss: 0.4082 - val_acc: 0.8155\n",
      "Epoch 29/60\n",
      "15216/15216 [==============================] - 3s 179us/step - loss: 0.4312 - acc: 0.8070 - val_loss: 0.4082 - val_acc: 0.8144\n",
      "Epoch 30/60\n",
      "15216/15216 [==============================] - 3s 185us/step - loss: 0.4309 - acc: 0.8059 - val_loss: 0.4079 - val_acc: 0.8155\n",
      "Epoch 31/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.4305 - acc: 0.8056 - val_loss: 0.4078 - val_acc: 0.8181\n",
      "Epoch 32/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4304 - acc: 0.8061 - val_loss: 0.4098 - val_acc: 0.8113\n",
      "Epoch 33/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4309 - acc: 0.8064 - val_loss: 0.4103 - val_acc: 0.8102\n",
      "Epoch 34/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4301 - acc: 0.8057 - val_loss: 0.4078 - val_acc: 0.8139\n",
      "Epoch 35/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4305 - acc: 0.8064 - val_loss: 0.4072 - val_acc: 0.8186\n",
      "Epoch 36/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4300 - acc: 0.8059 - val_loss: 0.4077 - val_acc: 0.8149\n",
      "Epoch 37/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4302 - acc: 0.8055 - val_loss: 0.4094 - val_acc: 0.8155\n",
      "Epoch 38/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.4301 - acc: 0.8068 - val_loss: 0.4084 - val_acc: 0.8139\n",
      "Epoch 39/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4297 - acc: 0.8062 - val_loss: 0.4075 - val_acc: 0.8144\n",
      "Epoch 40/60\n",
      "15216/15216 [==============================] - 2s 151us/step - loss: 0.4299 - acc: 0.8059 - val_loss: 0.4073 - val_acc: 0.8155\n",
      "Epoch 41/60\n",
      "15216/15216 [==============================] - 2s 150us/step - loss: 0.4297 - acc: 0.8056 - val_loss: 0.4085 - val_acc: 0.8155\n",
      "Epoch 42/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.4296 - acc: 0.8060 - val_loss: 0.4095 - val_acc: 0.8160\n",
      "Epoch 43/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4295 - acc: 0.8067 - val_loss: 0.4077 - val_acc: 0.8176\n",
      "Epoch 44/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4292 - acc: 0.8062 - val_loss: 0.4068 - val_acc: 0.8139\n",
      "Epoch 45/60\n",
      "15216/15216 [==============================] - 2s 153us/step - loss: 0.4296 - acc: 0.8070 - val_loss: 0.4079 - val_acc: 0.8144\n",
      "Epoch 46/60\n",
      "15216/15216 [==============================] - 2s 152us/step - loss: 0.4296 - acc: 0.8066 - val_loss: 0.4072 - val_acc: 0.8155\n",
      "Epoch 47/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.4291 - acc: 0.8065 - val_loss: 0.4068 - val_acc: 0.8181\n",
      "Epoch 48/60\n",
      "15216/15216 [==============================] - 2s 157us/step - loss: 0.4291 - acc: 0.8072 - val_loss: 0.4068 - val_acc: 0.8170\n",
      "Epoch 49/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.4293 - acc: 0.8063 - val_loss: 0.4064 - val_acc: 0.8176\n",
      "Epoch 50/60\n",
      "15216/15216 [==============================] - 3s 169us/step - loss: 0.4293 - acc: 0.8065 - val_loss: 0.4068 - val_acc: 0.8155\n",
      "Epoch 51/60\n",
      "15216/15216 [==============================] - 2s 159us/step - loss: 0.4292 - acc: 0.8060 - val_loss: 0.4073 - val_acc: 0.8144\n",
      "Epoch 52/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.4291 - acc: 0.8065 - val_loss: 0.4067 - val_acc: 0.8149\n",
      "Epoch 53/60\n",
      "15216/15216 [==============================] - 2s 147us/step - loss: 0.4291 - acc: 0.8053 - val_loss: 0.4075 - val_acc: 0.8160\n",
      "Epoch 54/60\n",
      "15216/15216 [==============================] - 2s 148us/step - loss: 0.4290 - acc: 0.8058 - val_loss: 0.4075 - val_acc: 0.8160\n",
      "Epoch 55/60\n",
      "15216/15216 [==============================] - 2s 154us/step - loss: 0.4290 - acc: 0.8062 - val_loss: 0.4080 - val_acc: 0.8155\n",
      "Epoch 56/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.4291 - acc: 0.8057 - val_loss: 0.4059 - val_acc: 0.8176\n",
      "Epoch 57/60\n",
      "15216/15216 [==============================] - 2s 163us/step - loss: 0.4288 - acc: 0.8067 - val_loss: 0.4065 - val_acc: 0.8139\n",
      "Epoch 58/60\n",
      "15216/15216 [==============================] - 3s 166us/step - loss: 0.4288 - acc: 0.8060 - val_loss: 0.4065 - val_acc: 0.8149\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15216/15216 [==============================] - 2s 155us/step - loss: 0.4287 - acc: 0.8064 - val_loss: 0.4068 - val_acc: 0.8165\n",
      "Epoch 60/60\n",
      "15216/15216 [==============================] - 2s 160us/step - loss: 0.4282 - acc: 0.8066 - val_loss: 0.4091 - val_acc: 0.8181\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU, Activation\n",
    "import numpy as np\n",
    "# mc = keras.callbacks.ModelCheckpoint('MAGIC-weights{epoch:08d}.h5', \n",
    "#                                      save_weights_only=True, period=5)\n",
    "model = Sequential()\n",
    "model.add(Dense(20,kernel_initializer='lecun_uniform',input_shape=(5,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(8, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "fvalue_selector = SelectKBest(f_classif, k=3)\n",
    "X_kbest = fvalue_selector.fit_transform(X_train.values, y_train.values)\n",
    "# X_train_dim = fvalue_selector.fit_transform(X_train, y_train)\n",
    "# X_val_dim = fvalue_selector.transform(X_val)\n",
    "\n",
    "# from sklearn.decomposition import FastICA\n",
    "# ica = FastICA(n_components=7)\n",
    "X_ICA = fvalue_selector.transform(X_train)\n",
    "X_ICA_val = fvalue_selector.transform(X_val)\n",
    "X_ICA_test = fvalue_selector.transform(X_test)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "km = GaussianMixture(n_components=5)\n",
    "km = km.fit(X_ICA)\n",
    "X_train_kmeans = km.predict_proba(X_ICA)\n",
    "X_val_kmeans = km.predict_proba(X_ICA_val)\n",
    "X_test_kmeans = km.predict_proba(X_ICA_test)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# X_train_kmeans = to_categorical(X_train_kmeans)\n",
    "# X_val_kmeans = to_categorical(X_val_kmeans)\n",
    "# X_test_kmeans = to_categorical(X_test_kmeans)\n",
    "\n",
    "# print(X_train_kmeans)\n",
    "hist = model.fit(X_train_kmeans, y_train.values, epochs=60, validation_data=(X_val_kmeans, y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8165089379600421\n"
     ]
    }
   ],
   "source": [
    "y_ = model.predict(X_test_kmeans)\n",
    "y_ = np.where(y_ > 0.5, 1, 0)\n",
    "accuracy = metrics.accuracy_score(y_test.values, y_)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
